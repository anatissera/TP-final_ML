{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e246ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naopersonal/Documents/Github/TP-final_ML/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Make sure you have this installed: pip install gdown\n",
    "import gdown\n",
    "import zipfile\n",
    "\n",
    "# Make sure you have copied the 'src' folder from FMM-Head into your project\n",
    "from src.datasets.datasetsLibrary import get_ptb_xl_fmm_dataset\n",
    "from pipeline_improved import VAE1D, beta_cyclic, compute_scores\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, accuracy_score, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d4a9d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74dba4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_data_subfolder(subfolder_name, start_path='.'):\n",
    "    current_path = os.path.abspath(start_path)\n",
    "    while True:\n",
    "        candidate = os.path.join(current_path, 'data', subfolder_name)\n",
    "        if os.path.isdir(candidate):\n",
    "            return candidate\n",
    "        parent = os.path.dirname(current_path)\n",
    "        if parent == current_path:\n",
    "            break\n",
    "        current_path = parent\n",
    "    return None\n",
    "\n",
    "DATA_DIR = find_data_subfolder('') # Finds the root 'data' folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d44dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FMM-enhanced PTB-XL dataset...\n",
      "Loading \"train\" folder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/95868 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/naopersonal/Documents/Github/TP-final_ML/data/ptb_xl_fmm/train/sample_12579_beat_9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/Github/TP-final_ML/ecg_project/src/datasets/datasetsLibrary.py:567\u001b[0m, in \u001b[0;36mget_ptb_xl_fmm_dataset.<locals>.get_numpy_dataset_from_slice\u001b[0;34m(slice)\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    566\u001b[0m     sample_coefficients \u001b[38;5;241m=\u001b[39m sample_coefficients \u001b[38;5;28;01mif\u001b[39;00m num_leads\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m12\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \\\n\u001b[0;32m--> 567\u001b[0m                     \u001b[43mextract_fmm_lead_from_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfmm_coefficients_array\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_coefficients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mlead\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlead\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_leads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mnum_waves\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_waves\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Fmm preprocessing from R code produces 8 leads\u001b[39;00m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;66;03m# predicted_sample_coefficients = predicted_sample_coefficients if num_leads==12 else \\\u001b[39;00m\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;66;03m#                 extract_fmm_lead_from_array(fmm_coefficients_array=predicted_sample_coefficients,\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;66;03m#                                                           lead=lead,num_leads=8,num_waves=num_waves)\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Github/TP-final_ML/ecg_project/src/utils/fmm.py:357\u001b[0m, in \u001b[0;36mextract_fmm_lead_from_array\u001b[0;34m(fmm_coefficients_array, lead, num_leads, num_waves)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(coeff_name\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m coeff_name\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeta\u001b[39m\u001b[38;5;124m\"\u001b[39m): \u001b[38;5;66;03m# For coefficients that have a different value for each lead\u001b[39;00m\n\u001b[0;32m--> 357\u001b[0m     single_lead_coeff_index \u001b[38;5;241m=\u001b[39m \u001b[43mstart_index\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mlead\u001b[49m\u001b[38;5;66;03m# Add the offset to get the coefficient for that lead\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(coeff_name\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m coeff_name\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124momega\u001b[39m\u001b[38;5;124m\"\u001b[39m): \u001b[38;5;66;03m# For coefficients that have the same value for each lead\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'int' and 'NoneType'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# This cell loads the FMM-enhanced dataset.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# If the data is not found in './data/ptb_xl_fmm', it will automatically download it.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# This may take a while the first time.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading FMM-enhanced PTB-XL dataset...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m data_dict \u001b[38;5;241m=\u001b[39m \u001b[43mget_ptb_xl_fmm_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatapath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDATA_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_leads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_waves\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelete_high_A\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset loaded successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Extract the training and testing sets\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Github/TP-final_ML/ecg_project/src/datasets/datasetsLibrary.py:590\u001b[0m, in \u001b[0;36mget_ptb_xl_fmm_dataset\u001b[0;34m(datapath, frequency, lead, delete_high_A, **kwargs)\u001b[0m\n\u001b[1;32m    588\u001b[0m train_folder_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# Modify to train12_predicted and test12_predicted for 12 leads instead of 8\u001b[39;00m\n\u001b[1;32m    589\u001b[0m test_folder_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 590\u001b[0m train_dict  \u001b[38;5;241m=\u001b[39m \u001b[43mget_numpy_dataset_from_slice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mslice\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_folder_name\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m    591\u001b[0m test_dict \u001b[38;5;241m=\u001b[39m get_numpy_dataset_from_slice(\u001b[38;5;28mslice\u001b[39m\u001b[38;5;241m=\u001b[39mtest_folder_name) \n\u001b[1;32m    593\u001b[0m coeffs_train \u001b[38;5;241m=\u001b[39m sort_fmm_coeffs_array(fmm_array\u001b[38;5;241m=\u001b[39mtrain_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoefficients\u001b[39m\u001b[38;5;124m\"\u001b[39m], num_leads\u001b[38;5;241m=\u001b[39mnum_leads, num_waves\u001b[38;5;241m=\u001b[39mnum_waves)\n",
      "File \u001b[0;32m~/Documents/Github/TP-final_ML/ecg_project/src/datasets/datasetsLibrary.py:576\u001b[0m, in \u001b[0;36mget_ptb_xl_fmm_dataset.<locals>.get_numpy_dataset_from_slice\u001b[0;34m(slice)\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    575\u001b[0m         \u001b[38;5;28mprint\u001b[39m(full_file_name)\n\u001b[0;32m--> 576\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m(\u001b[38;5;167;01mValueError\u001b[39;00m)\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;66;03m# coefficients_circular[row_number] = angle_vector_to_cos_sin(sample_coefficients,circular_indexes)\u001b[39;00m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;66;03m# Before returning, delete rows which have too high A parameter\u001b[39;00m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(delete_high_A):\n",
      "\u001b[0;31mValueError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# This cell loads the FMM-enhanced dataset.\n",
    "# If the data is not found in './data/ptb_xl_fmm', it will automatically download it.\n",
    "# This may take a while the first time.\n",
    "\n",
    "print(\"Loading FMM-enhanced PTB-XL dataset...\")\n",
    "# Corrected call (added lead=0)\n",
    "data_dict = get_ptb_xl_fmm_dataset(\n",
    "    datapath=DATA_DIR,\n",
    "    num_leads=1,\n",
    "    lead=0,  # <-- ADD THIS LINE\n",
    "    num_waves=5,\n",
    "    sequence_length=2048,\n",
    "    delete_high_A=False\n",
    ")\n",
    "print(\"Dataset loaded successfully.\")\n",
    "\n",
    "# Extract the training and testing sets\n",
    "X_train_raw = data_dict['train']['data']\n",
    "y_train = data_dict['train']['labels']\n",
    "coeffs_train = data_dict['train']['coefficients']\n",
    "\n",
    "X_test_raw = data_dict['test']['data'] \n",
    "y_test = data_dict['test']['labels']\n",
    "coeffs_test = data_dict['test']['coefficients']\n",
    "\n",
    "# --- Create Multi-Channel Input ---\n",
    "# The VAE will be trained on the signal (1 channel) + its FMM features (55 channels).\n",
    "# We reshape the 55 coefficients to match the signal's time dimension (2048).\n",
    "train_coeffs_reshaped = np.repeat(np.expand_dims(coeffs_train, axis=1), 2048, axis=1)\n",
    "test_coeffs_reshaped = np.repeat(np.expand_dims(coeffs_test, axis=1), 2048, axis=1)\n",
    "\n",
    "# Concatenate along the channel dimension\n",
    "X_train = np.concatenate([X_train_raw, train_coeffs_reshaped], axis=2).astype(np.float32)\n",
    "X_test = np.concatenate([X_test_raw, test_coeffs_reshaped], axis=2).astype(np.float32)\n",
    "\n",
    "# --- Separate into Normal and Anomalous Sets ---\n",
    "# We use the provided labels to create our final sets.\n",
    "normal_class_id = data_dict['params']['normal_class']\n",
    "train_normals = X_train[y_train == normal_class_id]\n",
    "train_anomalies = X_train[y_train != normal_class_id]\n",
    "test_normals = X_test[y_test == normal_class_id]\n",
    "test_anomalies = X_test[y_test != normal_class_id]\n",
    "\n",
    "# --- Convert to PyTorch Tensors and fix dimensions for Conv1D ---\n",
    "# Conv1D expects (Batch, Channels, Length)\n",
    "dev_norm_tensor = torch.tensor(train_normals).permute(0, 2, 1)\n",
    "ano_dev_tensor = torch.tensor(train_anomalies).permute(0, 2, 1)\n",
    "test_norm_tensor = torch.tensor(test_normals).permute(0, 2, 1)\n",
    "ano_test_tensor = torch.tensor(test_anomalies).permute(0, 2, 1)\n",
    "\n",
    "print(f\"Shape of multi-channel normal data for VAE training: {dev_norm_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b07b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normals = load_sanos(PTB_DIR, CHAP_DIR)     \n",
    "#ptb_df  = pd.read_csv(find_file(PTB_DIR, 'ptbxl_database.csv')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8fc85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#anomalies = load_all_anomalies(PTB_DIR, CHAP_DIR, ptb_df)  # (N_ano,1,L) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeffd1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE & Training Hyperparameters\n",
    "latent_dim = 32\n",
    "lr = 1e-3\n",
    "n_blocks = 3\n",
    "epochs = 50  # Increase this (e.g., to 100-150) for better results if you have time\n",
    "batch_size = 32\n",
    "beta_value = 1.0 # Using a constant beta of 1.0 is a good starting point\n",
    "NUM_CHANNELS = dev_norm_tensor.shape[1] # Should be 56 (1 ECG + 55 FMM)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67870d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- VAE Training on Normal Data Only ---\n",
    "# The VAE learns the distribution of healthy, FMM-enhanced signals.\n",
    "dev_ds = TensorDataset(dev_norm_tensor, torch.zeros(len(dev_norm_tensor)))\n",
    "dev_loader = DataLoader(dev_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = VAE1D(input_ch=NUM_CHANNELS, latent_dim=latent_dim, n_blocks=n_blocks).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "print(\"Starting VAE training...\")\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for (x, _) in tqdm(dev_loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False):\n",
    "        x = x.to(device)\n",
    "        mu, logv = model.encode(x)\n",
    "        z = model.reparameterize(mu, logv)\n",
    "        rec = model.decode(z)\n",
    "        \n",
    "        recon_loss = nn.functional.mse_loss(rec, x)\n",
    "        kl_loss = (-0.5 * (1 + logv - mu.pow(2) - logv.exp()).sum()) / x.size(0)\n",
    "        \n",
    "        loss = recon_loss + beta_value * kl_loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}/{epochs}, VAE Loss: {total_loss/len(dev_loader):.4f}\")\n",
    "\n",
    "# Save the trained VAE model\n",
    "torch.save(model.state_dict(), 'fmm_vae_model.pth')\n",
    "print(\"VAE model saved to fmm_vae_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb2d9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- XGBoost Classifier Training ---\n",
    "\n",
    "# 1. Prepare a mixed development set for the classifier\n",
    "dev_x_classifier = torch.cat([dev_norm_tensor, ano_dev_tensor], dim=0)\n",
    "y_dev_classifier = torch.tensor(\n",
    "    np.concatenate([np.zeros(len(dev_norm_tensor)), np.ones(len(ano_dev_tensor))]),\n",
    "    dtype=torch.long\n",
    ")\n",
    "dev_loader_classifier = DataLoader(TensorDataset(dev_x_classifier, y_dev_classifier), batch_size=batch_size)\n",
    "\n",
    "# 2. Extract features using the trained VAE\n",
    "print(\"Extracting features from dev set for classifier training...\")\n",
    "errs_dev, zs_dev = compute_scores(model, dev_loader_classifier, device, beta=beta_value)\n",
    "max_val = np.finfo(np.float32).max / 10\n",
    "errs_dev = np.nan_to_num(errs_dev, nan=0.0, posinf=max_val, neginf=-max_val)\n",
    "zs_dev = np.nan_to_num(zs_dev, nan=0.0, posinf=max_val, neginf=-max_val)\n",
    "X_train_clf = np.hstack([errs_dev.reshape(-1, 1), zs_dev])\n",
    "y_train_clf = y_dev_classifier.numpy()\n",
    "\n",
    "# 3. Train the XGBoost Classifier\n",
    "print(\"Training XGBoost classifier...\")\n",
    "clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "clf.fit(X_train_clf, y_train_clf)\n",
    "print(\"Classifier training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61eaef76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 3.2853\n",
      "Epoch 2/50, Loss: 1.0379\n",
      "Epoch 3/50, Loss: 1.0125\n",
      "Epoch 4/50, Loss: 1.0056\n",
      "Epoch 5/50, Loss: 1.0046\n",
      "Epoch 6/50, Loss: 1.0032\n",
      "Epoch 7/50, Loss: 1.0017\n",
      "Epoch 8/50, Loss: 0.9973\n",
      "Epoch 9/50, Loss: 0.9988\n",
      "Epoch 10/50, Loss: 1.0093\n",
      "Epoch 11/50, Loss: 1.0167\n",
      "Epoch 12/50, Loss: 0.9975\n",
      "Epoch 13/50, Loss: 0.9910\n",
      "Epoch 14/50, Loss: 0.9896\n",
      "Epoch 15/50, Loss: 0.9888\n",
      "Epoch 16/50, Loss: 0.9887\n",
      "Epoch 17/50, Loss: 0.9887\n",
      "Epoch 18/50, Loss: 0.9887\n",
      "Epoch 19/50, Loss: 0.9892\n",
      "Epoch 20/50, Loss: 0.9906\n",
      "Epoch 21/50, Loss: 0.9922\n",
      "Epoch 22/50, Loss: 1.0031\n",
      "Epoch 23/50, Loss: 0.9910\n",
      "Epoch 24/50, Loss: 0.9905\n",
      "Epoch 25/50, Loss: 0.9888\n",
      "Epoch 26/50, Loss: 0.9879\n",
      "Epoch 27/50, Loss: 0.9877\n",
      "Epoch 28/50, Loss: 0.9877\n",
      "Epoch 29/50, Loss: 0.9879\n",
      "Epoch 30/50, Loss: 0.9876\n",
      "Epoch 31/50, Loss: 0.9877\n",
      "Epoch 32/50, Loss: 0.9880\n",
      "Epoch 33/50, Loss: 0.9880\n",
      "Epoch 34/50, Loss: 0.9882\n",
      "Epoch 35/50, Loss: 0.9879\n",
      "Epoch 36/50, Loss: 0.9879\n",
      "Epoch 37/50, Loss: 0.9877\n",
      "Epoch 38/50, Loss: 16.7783\n",
      "Epoch 39/50, Loss: 1.1057\n",
      "Epoch 40/50, Loss: 1.0413\n",
      "Epoch 41/50, Loss: 1.0213\n",
      "Epoch 42/50, Loss: 1.0121\n",
      "Epoch 43/50, Loss: 1.0070\n",
      "Epoch 44/50, Loss: 1.0029\n",
      "Epoch 45/50, Loss: 1.0005\n",
      "Epoch 46/50, Loss: 0.9992\n",
      "Epoch 47/50, Loss: 0.9973\n",
      "Epoch 48/50, Loss: 0.9958\n",
      "Epoch 49/50, Loss: 0.9953\n",
      "Epoch 50/50, Loss: 0.9948\n",
      "Model saved to final_vae_model.pth\n"
     ]
    }
   ],
   "source": [
    "# --- Final Evaluation on Unseen Test Set ---\n",
    "\n",
    "# 1. Prepare the final, balanced test set\n",
    "N_test = len(test_norm_tensor)\n",
    "final_test_x = torch.cat([test_norm_tensor, ano_test_tensor[:N_test]], dim=0)\n",
    "final_y_test = torch.tensor(\n",
    "    np.concatenate([np.zeros(N_test), np.ones(N_test)]),\n",
    "    dtype=torch.long\n",
    ")\n",
    "final_test_loader = DataLoader(TensorDataset(final_test_x, final_y_test), batch_size=batch_size)\n",
    "\n",
    "# 2. Extract features from the test set\n",
    "print(\"Extracting features from the unseen test set...\")\n",
    "errs_test, zs_test = compute_scores(model, final_test_loader, device, beta=beta_value)\n",
    "errs_test = np.nan_to_num(errs_test, nan=0.0, posinf=max_val, neginf=-max_val)\n",
    "zs_test = np.nan_to_num(zs_test, nan=0.0, posinf=max_val, neginf=-max_val)\n",
    "X_test_clf = np.hstack([errs_test.reshape(-1, 1), zs_test])\n",
    "y_test_clf = final_y_test.numpy()\n",
    "\n",
    "# 3. Make predictions and evaluate\n",
    "print(\"Evaluating classifier on the unseen test set...\")\n",
    "probs = clf.predict_proba(X_test_clf)[:, 1]\n",
    "y_pred = (probs >= 0.5).astype(int)\n",
    "\n",
    "metrics = {\n",
    "    'roc_auc': roc_auc_score(y_test_clf, probs),\n",
    "    'precision': precision_score(y_test_clf, y_pred),\n",
    "    'recall': recall_score(y_test_clf, y_pred),\n",
    "    'f1': f1_score(y_test_clf, y_pred),\n",
    "    'accuracy': accuracy_score(y_test_clf, y_pred),\n",
    "    'r2': r2_score(y_test_clf, y_pred) # R2 score on binarized predictions\n",
    "}\n",
    "print(\"\\n--- Final Metrics on Unseen Test Set ---\")\n",
    "print(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
