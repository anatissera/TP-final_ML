{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3e246ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d4a9d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28f80f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline2 import load_normals, train_cvae, get_errors, find_file, get_anomalies, evaluate_all, find_data_subfolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "838bc113",
   "metadata": {},
   "outputs": [],
   "source": [
    "PTB_DIR = find_data_subfolder('ptb-xl/1.0.3')\n",
    "CHAP_DIR = find_data_subfolder('ChapmanShaoxing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7015b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Carga normales y entrena\n",
    "normals = load_normals()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e1a86e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anapt\\Repositorios\\TP-final_ML\\TP-final_ML\\ecg_project\\pipeline2.py:66: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:257.)\n",
      "  train_loader = DataLoader(TensorDataset(torch.tensor(train_set)), batch_size=BATCH, shuffle=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0240ba553dc54a39a0ec827f16cebbeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Ep1/18:   0%|          | 0/293 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss=0.9993 — Val MAE=0.6029\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8e6ed24d4f4451daa7fb28b37fc2a56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Ep2/18:   0%|          | 0/293 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss=0.9911 — Val MAE=0.6029\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6af2f97526e401d8bc8cd75f4e4eb21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Ep3/18:   0%|          | 0/293 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss=0.9911 — Val MAE=0.6029\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dad24af483146fc8903feef33e347d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Ep4/18:   0%|          | 0/293 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss=0.9911 — Val MAE=0.6029\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "279ad9519325474cb4f7d68254cff307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Ep5/18:   0%|          | 0/293 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss=0.9911 — Val MAE=0.6029\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00825fbfc3644acd971ddd2743d82654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Ep6/18:   0%|          | 0/293 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss=0.9911 — Val MAE=0.6029\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f08457b0fa2407fbfa700dcf1d963bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Ep7/18:   0%|          | 0/293 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss=0.9911 — Val MAE=0.6029\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cbf21b6acc444aab789aa0cd1252d4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Ep8/18:   0%|          | 0/293 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss=0.9911 — Val MAE=0.6029\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d6b878bdcc64a5eb0ec1e6489ba0dfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Ep9/18:   0%|          | 0/293 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss=0.9911 — Val MAE=0.6029\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b9a24c4098448e6b6f9e2ff6962c471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Ep10/18:   0%|          | 0/293 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss=0.9911 — Val MAE=0.6029\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2306ece9c15c4e37ab4cf11cba1359f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Ep11/18:   0%|          | 0/293 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss=0.9911 — Val MAE=0.6029\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ff31763150b428fa3fe1b92b261ac7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Ep12/18:   0%|          | 0/293 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss=0.9911 — Val MAE=0.6029\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "952054748bbe4ff1b3959cede9ce7c55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Ep13/18:   0%|          | 0/293 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Loss=0.9911 — Val MAE=0.6029\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcb8d87b50754578955af3cf29d7a8c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Ep14/18:   0%|          | 0/293 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Loss=0.9911 — Val MAE=0.6029\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9de32f5405045f5b48954031fa9e02c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Ep15/18:   0%|          | 0/293 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Loss=0.9911 — Val MAE=0.6029\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9660a24b8fb40c4b85ff041978c1a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Ep16/18:   0%|          | 0/293 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Train Loss=0.9911 — Val MAE=0.6029\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "207a1bbc9456426eb6b57ab10de1c107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Ep17/18:   0%|          | 0/293 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Train Loss=0.9911 — Val MAE=0.6029\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "648325da07cc4e4887aca5874bca4d28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Ep18/18:   0%|          | 0/293 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Train Loss=0.9911 — Val MAE=0.6029\n"
     ]
    }
   ],
   "source": [
    "model, val_loader, device = train_cvae(normals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbb2e9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Errores sanos\n",
    "healthy_errors = get_errors(model, val_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f89df732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anómalos PTB-XL: 21799\n",
      "Anómalos Chapman: 10247\n"
     ]
    }
   ],
   "source": [
    "# 3) Anómalos\n",
    "df_meta = pd.read_csv(find_file(PTB_DIR, 'ptbxl_database.csv'))\n",
    "ptb_errors, chap_errors = get_anomalies(model, df_meta, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "348eddc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.473\n",
      "PR  AUC: 0.933\n",
      "Best threshold (Youden): 0.7031\n",
      "Metrics saved to metrics_combined_lead_II.json\n"
     ]
    }
   ],
   "source": [
    "# 4) Evaluación final\n",
    "evaluate_all(healthy_errors, ptb_errors, chap_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7e6070",
   "metadata": {},
   "source": [
    "Improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e9250e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "\n",
    "from preprocess import (\n",
    "    load_sanos,\n",
    "    load_ptbxl_anomalies,\n",
    "    load_chapman_anomalies,\n",
    "    load_mitbih\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "11fcf085",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "baeee008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_file(root, filename):\n",
    "    \"\"\"\n",
    "    Busca recursivamente `filename` bajo `root` y devuelve la ruta completa.\n",
    "    \"\"\"\n",
    "    for r, _, files in os.walk(root):\n",
    "        if filename in files:\n",
    "            return os.path.join(r, filename)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401c4874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Cargar datos\n",
    "normals = load_sanos() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "722c80fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# anomalías PTB y Chapman\n",
    "from preprocess import load_ptbxl, pd\n",
    "\n",
    "\n",
    "ptb_df = pd.read_csv(find_file(PTB_DIR, 'ptbxl_database.csv'))\n",
    "# ptb_df = pd.read_csv(os.path.join(os.getcwd(), 'ptbxl_database.csv'))\n",
    "anom_ptb = load_ptbxl_anomalies(PTB_DIR, ptb_df)\n",
    "# anom_ptb = load_ptbxl_anomalies(os.getcwd(), ptb_df)\n",
    "anom_chap = load_chapman_anomalies(CHAP_DIR)\n",
    "\n",
    "# anom_mit = load_mitbih(os.getcwd())\n",
    "anomalies = np.concatenate([anom_ptb, anom_chap], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9ac49677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Preparar DataLoaders\n",
    "X_norm = torch.tensor(normals[:,1:2,:]).unsqueeze(1)  # lead II\n",
    "X_ano  = torch.tensor(anomalies[:,1:2,:]).unsqueeze(1)\n",
    "y_norm = np.zeros(len(X_norm))\n",
    "y_ano  = np.ones(len(X_ano))\n",
    "\n",
    "dataset_norm = TensorDataset(X_norm, torch.tensor(y_norm))\n",
    "dataset_ano  = TensorDataset(X_ano,  torch.tensor(y_ano))\n",
    "# split train/val/test: 70/10/20 para sanos, usar mismo tamaño para anómalos en test\n",
    "n = len(dataset_norm)\n",
    "i1, i2 = int(0.7*n), int(0.8*n)\n",
    "ds_train = torch.utils.data.Subset(dataset_norm, range(0,i1))\n",
    "ds_val   = torch.utils.data.Subset(dataset_norm, range(i1,i2))\n",
    "ds_test  = torch.utils.data.Subset(dataset_norm, range(i2,n))\n",
    "# añadir anomalías SOLO en test para evaluar\n",
    "ds_test = torch.utils.data.ConcatDataset([ds_test,\n",
    "                                            torch.utils.data.Subset(dataset_ano, range(0, len(ds_test)))])\n",
    "# DataLoaders\n",
    "loader_train = DataLoader(ds_train, batch_size=32, shuffle=True)\n",
    "loader_val   = DataLoader(ds_val,   batch_size=32, shuffle=False)\n",
    "loader_test  = DataLoader(ds_test,  batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ab748bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline_improved import VAE1D, beta_cyclic, compute_anomaly_scores, fit_mahalanobis, mahalanobis_distance, grid_search_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "9840b0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3) Modelo y optimizador\n",
    "model = VAE1D(input_ch=1, latent_dim=32, seq_len=2048).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "74dba4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset, Subset, ConcatDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "dd502cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: X_norm shape: torch.Size([11716, 0, 2048]), X_ano shape: torch.Size([20330, 0, 2048])\n",
      "DEBUG: len train: 8201, len val: 1171, len testN: 2344\n"
     ]
    }
   ],
   "source": [
    "# 2) Preparar TensorDataset de lead II\n",
    "X_norm = torch.tensor(normals[:,1:2,:], dtype=torch.float32)\n",
    "y_norm = torch.zeros(len(X_norm), dtype=torch.long)\n",
    "X_ano  = torch.tensor(anomalies[:,1:2,:], dtype=torch.float32)\n",
    "y_ano  = torch.ones(len(X_ano), dtype=torch.long)\n",
    "\n",
    "print(f\"DEBUG: X_norm shape: {X_norm.shape}, X_ano shape: {X_ano.shape}\")\n",
    "\n",
    "ds_norm = TensorDataset(X_norm, y_norm)\n",
    "ds_ano  = TensorDataset(X_ano,  y_ano)\n",
    "\n",
    "# Splits sanos 70/10/20\n",
    "n = len(ds_norm)\n",
    "i1, i2 = int(0.7*n), int(0.8*n)\n",
    "ds_train = Subset(ds_norm, range(0,i1))\n",
    "ds_val   = Subset(ds_norm, range(i1,i2))\n",
    "ds_testN = Subset(ds_norm, range(i2,n))\n",
    "\n",
    "# DEBUG: longitudes de los splits\n",
    "print(f\"DEBUG: len train: {len(ds_train)}, len val: {len(ds_val)}, len testN: {len(ds_testN)}\")\n",
    "n = len(ds_norm)\n",
    "i1, i2 = int(0.7*n), int(0.8*n)\n",
    "ds_train = Subset(ds_norm, range(i1))\n",
    "ds_val   = Subset(ds_norm, range(i1, i2))\n",
    "ds_testN = Subset(ds_norm, range(i2, n))\n",
    "\n",
    "# Anomalías igual tamaño para test\n",
    "ds_testA = Subset(ds_ano, range(len(ds_testN)))\n",
    "ds_test  = ConcatDataset([ds_testN, ds_testA])\n",
    "\n",
    "loader_train = DataLoader(ds_train, batch_size=32, shuffle=True)\n",
    "loader_val   = DataLoader(ds_val,   batch_size=32, shuffle=False)\n",
    "loader_test  = DataLoader(ds_test,  batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "90188bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: X_norm shape: torch.Size([11716, 1, 2048]), X_ano shape: torch.Size([20330, 1, 2048])\n",
      "DEBUG: len train: 8201, len val: 1171, len testN: 2344\n",
      "Ep 1/20 - Loss: 5.2783 - β: 0.44\n",
      "Ep 2/20 - Loss: 1.7041 - β: 0.89\n",
      "Ep 3/20 - Loss: 1.4397 - β: 1.33\n",
      "Ep 4/20 - Loss: 1.2862 - β: 1.78\n",
      "Ep 5/20 - Loss: 1.1993 - β: 2.22\n",
      "Ep 6/20 - Loss: 1.1437 - β: 2.67\n",
      "Ep 7/20 - Loss: 1.1050 - β: 3.11\n",
      "Ep 8/20 - Loss: 1.0739 - β: 3.56\n",
      "Ep 9/20 - Loss: 1.0526 - β: 4.00\n",
      "Ep 10/20 - Loss: 0.9931 - β: 0.00\n",
      "Ep 11/20 - Loss: 1.1948 - β: 0.44\n",
      "Ep 12/20 - Loss: 1.0010 - β: 0.89\n",
      "Ep 13/20 - Loss: 1.0040 - β: 1.33\n",
      "Ep 14/20 - Loss: 1.0034 - β: 1.78\n",
      "Ep 15/20 - Loss: 0.9995 - β: 2.22\n",
      "Ep 16/20 - Loss: 0.9981 - β: 2.67\n",
      "Ep 17/20 - Loss: 0.9954 - β: 3.11\n",
      "Ep 18/20 - Loss: 0.9947 - β: 3.56\n",
      "Ep 19/20 - Loss: 0.9934 - β: 4.00\n",
      "Ep 20/20 - Loss: 0.9923 - β: 0.00\n"
     ]
    }
   ],
   "source": [
    "# 2) Preparar TensorDataset de lead II\n",
    "X_norm = torch.tensor(normals, dtype=torch.float32)  # normals shape already (N,1,2048)\n",
    "y_norm = torch.zeros(len(X_norm), dtype=torch.long)\n",
    "X_ano  = torch.tensor(anomalies, dtype=torch.float32)  # anomalies shape (M,1,2048)\n",
    "y_ano  = torch.ones(len(X_ano), dtype=torch.long)\n",
    "\n",
    "print(f\"DEBUG: X_norm shape: {X_norm.shape}, X_ano shape: {X_ano.shape}\")\n",
    "\n",
    "ds_norm = TensorDataset(X_norm, y_norm)\n",
    "ds_ano  = TensorDataset(X_ano,  y_ano)\n",
    "\n",
    "# Splits sanos 70/10/20\n",
    "n = len(ds_norm)\n",
    "i1, i2 = int(0.7*n), int(0.8*n)\n",
    "ds_train = Subset(ds_norm, range(0, i1))\n",
    "ds_val   = Subset(ds_norm, range(i1, i2))\n",
    "ds_testN = Subset(ds_norm, range(i2, n))\n",
    "\n",
    "# DEBUG: longitudes de los splits\n",
    "print(f\"DEBUG: len train: {len(ds_train)}, len val: {len(ds_val)}, len testN: {len(ds_testN)}\")\n",
    "\n",
    "# Anomalías igual tamaño para validación y test\n",
    "ds_valA = Subset(ds_ano, range(len(ds_val)))\n",
    "ds_testA = Subset(ds_ano, range(len(ds_testN)))\n",
    "\n",
    "# Combina normales y anomalías en val y test\n",
    "ds_val_all  = ConcatDataset([ds_val,  ds_valA])\n",
    "ds_test_all = ConcatDataset([ds_testN, ds_testA])\n",
    "\n",
    "loader_train = DataLoader(ds_train,    batch_size=32, shuffle=True)\n",
    "loader_val   = DataLoader(ds_val_all,  batch_size=32, shuffle=False)\n",
    "loader_test  = DataLoader(ds_test_all, batch_size=32, shuffle=False)\n",
    "ds_testA = Subset(ds_ano, range(len(ds_testN)))\n",
    "ds_test  = ConcatDataset([ds_testN, ds_testA])\n",
    "\n",
    "loader_train = DataLoader(ds_train, batch_size=32, shuffle=True)\n",
    "loader_val   = DataLoader(ds_val,   batch_size=32, shuffle=False)\n",
    "loader_test  = DataLoader(ds_test,  batch_size=32, shuffle=False)\n",
    "\n",
    "# 3) Modelo y optimizador\n",
    "model = VAE1D(input_ch=1, latent_dim=64, seq_len=2048).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# 4) Entrenamiento\n",
    "# Ajustes: LR reducido y gradient clipping para estabilizar\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "epochs = 20\n",
    "for ep in range(1, epochs+1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, _ in loader_train:\n",
    "        x = x.to(device)\n",
    "        rec, mu, logvar = model(x)\n",
    "        recon_loss = torch.mean((rec - x)**2)\n",
    "        kl_loss = -0.5 * torch.sum(1 + logvar - mu**2 - torch.exp(logvar)) / x.size(0)\n",
    "        beta = beta_cyclic(ep)\n",
    "        loss = recon_loss + beta * kl_loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Ep {ep}/{epochs} - Loss: {total_loss/len(loader_train):.4f} - β: {beta:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "841a7042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0, 'auc': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\TP-final_ML\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\envs\\TP-final_ML\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\envs\\TP-final_ML\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\envs\\TP-final_ML\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\envs\\TP-final_ML\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 5) Evaluación: scores, mahala, grid-search\n",
    "from pprint import pprint\n",
    "scores_val = compute_anomaly_scores(model, loader_val, device)\n",
    "mu_norm_val = scores_val['mu']\n",
    "mbar, inv = fit_mahalanobis(mu_norm_val)\n",
    "dM_val = mahalanobis_distance(mu_norm_val, mbar, inv)\n",
    "best = grid_search_alpha(scores_val['mse'], dM_val, loader_val.dataset[:][1])\n",
    "pprint(best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "7db2d2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Detectores no supervisados e inferencia en test\n",
    "iso = IsolationForest(contamination='auto').fit(mu_val)\n",
    "\n",
    "ocsvm = OneClassSVM(gamma='auto', nu=0.01).fit(mu_norm_val)\n",
    "\n",
    "scores_test = compute_anomaly_scores(model, loader_test, device)\n",
    "mu_test = scores_test['mu']\n",
    "dM_test = mahalanobis_distance(mu_test, mbar, inv)\n",
    "comb_test = best['alpha']*scores_test['mse'] + (1-best['alpha'])*dM_test\n",
    "iso_s = -iso.decision_function(mu_test)\n",
    "oc_s  = -ocsvm.decision_function(mu_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "e7359c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC ELBO: 0.5015\n",
      "AUC Mahala: 0.2605\n",
      "AUC Combined: 0.2605\n",
      "AUC IF: 0.5154\n",
      "AUC OC: 0.3669\n"
     ]
    }
   ],
   "source": [
    "# 7) Balancear para métricas\n",
    "y_test = np.concatenate([np.zeros(len(ds_test.datasets[0])), np.ones(len(ds_test.datasets[1]))])\n",
    "idx_n = np.where(y_test==0)[0]\n",
    "idx_a = np.where(y_test==1)[0]\n",
    "sel_a = np.random.choice(idx_a, size=len(idx_n), replace=False)\n",
    "idx = np.concatenate([idx_n, sel_a])\n",
    "for name, sc in [('ELBO',scores_test['score']),\n",
    "                    ('Mahala',dM_test),('Combined',comb_test),\n",
    "                    ('IF',iso_s),('OC',oc_s)]:\n",
    "    print(f\"AUC {name}: {roc_auc_score(y_test[idx], sc[idx]):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c617d075",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47a0b3f2",
   "metadata": {},
   "source": [
    "otro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "b1b07b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "norms = normals\n",
    "ptb   = anom_ptb\n",
    "chap  = anom_chap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "0c1111aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "anos  = np.concatenate([ptb, chap], axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "b8f77f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to tensors with proper channel dim\n",
    "X_norm = torch.tensor(norms[:,1:2,:], dtype=torch.float32)\n",
    "X_ano  = torch.tensor(anos[:,1:2,:], dtype=torch.float32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "b82cc2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split normals into train/val\n",
    "n_norm = len(X_norm)\n",
    "i1 = int(0.6 * n_norm)\n",
    "i2 = int(0.8 * n_norm)\n",
    "train_norm = X_norm[:i1]\n",
    "val_norm   = X_norm[i1:i2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "12076caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline_improved import beta_linear, compute_scores, XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7205c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [32, 1, 19], expected input[32, 0, 2048] to have 1 channels, but got 0 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[180], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, _ \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     22\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 23\u001b[0m     mu, logv \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     z \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mreparameterize(mu, logv)\n\u001b[0;32m     25\u001b[0m     rec \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdecode(z)\n",
      "File \u001b[1;32mc:\\Users\\anapt\\Repositorios\\TP-final_ML\\TP-final_ML\\ecg_project\\pipeline_improved.py:65\u001b[0m, in \u001b[0;36mVAE1D.encode\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 65\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_mu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten(h)), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_logv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten(h))\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\TP-final_ML\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\TP-final_ML\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\TP-final_ML\\lib\\site-packages\\torch\\nn\\modules\\container.py:240\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 240\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\TP-final_ML\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\TP-final_ML\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\TP-final_ML\\lib\\site-packages\\torch\\nn\\modules\\conv.py:375\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\TP-final_ML\\lib\\site-packages\\torch\\nn\\modules\\conv.py:370\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(\n\u001b[0;32m    360\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    361\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    369\u001b[0m     )\n\u001b[1;32m--> 370\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [32, 1, 19], expected input[32, 0, 2048] to have 1 channels, but got 0 channels instead"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert to tensors with proper channel dim (Lead II)\n",
    "# load_sanos() returns shape (n_samples, n_leads, seq_len)\n",
    "# Select lead II (index 1)\n",
    "norms_lead = norms[:, 1, :]\n",
    "anos_lead = anos[:, 1, :]\n",
    "# Make shape (n_samples, 1, seq_len)\n",
    "X_norm = torch.tensor(norms_lead, dtype=torch.float32).unsqueeze(1)\n",
    "X_ano  = torch.tensor(anos_lead, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Split normals into train/val\n",
    "n_norm = len(X_norm)\n",
    "i1 = int(0.6 * n_norm)\n",
    "i2 = int(0.8 * n_norm)\n",
    "train_norm = X_norm[:i1]\n",
    "val_norm   = X_norm[i1:i2]\n",
    "\n",
    "# Hyperparameter grid\n",
    "grid = {\n",
    "    'latent_dim': [8, 16, 32],\n",
    "    'lr': [1e-3, 1e-4],\n",
    "    'n_blocks': [2, 3],\n",
    "    'beta_fn': [('linear', beta_linear), ('cyclic', beta_cyclic)]\n",
    "}\n",
    "results = []\n",
    "\n",
    "for ld in grid['latent_dim']:\n",
    "    for lr in grid['lr']:\n",
    "        for nb in grid['n_blocks']:\n",
    "            for name, beta_fn in grid['beta_fn']:\n",
    "                # a) Train VAE on normals only\n",
    "                train_ds = TensorDataset(train_norm, torch.zeros(len(train_norm)))\n",
    "                train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "                model = VAE1D(input_ch=1, latent_dim=ld, n_blocks=nb).to(device)\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "                for epoch in range(10):\n",
    "                    model.train()\n",
    "                    for x, _ in train_loader:\n",
    "                        x = x.to(device)\n",
    "                        mu, logv = model.encode(x)\n",
    "                        z = model.reparameterize(mu, logv)\n",
    "                        rec = model.decode(z)\n",
    "                        recon_loss = ((rec - x)**2).mean()\n",
    "                        kl_loss = (-0.5 * (1 + logv - mu.pow(2) - logv.exp()).sum())/x.size(0)\n",
    "                        beta = beta_fn(epoch, 10)\n",
    "                        loss = recon_loss + beta * kl_loss\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # b) Prepare validation set: equal normals and anomalies\n",
    "                N = len(val_norm)\n",
    "                val_ano = X_ano[:N]\n",
    "                X_val = torch.cat([val_norm, val_ano], dim=0)\n",
    "                y_val = np.concatenate([np.zeros(N), np.ones(N)])\n",
    "                val_ds = TensorDataset(X_val, torch.tensor(y_val, dtype=torch.long))\n",
    "                val_loader = DataLoader(val_ds, batch_size=32, shuffle=False)\n",
    "\n",
    "                # c) Compute scores on validation\n",
    "                beta_last = beta_fn(9, 10)\n",
    "                errs, zs = compute_scores(model, val_loader, device, beta_last)\n",
    "\n",
    "                # d) Mahalanobis\n",
    "                mu_bar = zs[:N].mean(axis=0)\n",
    "                cov = np.cov(zs[:N].T) + 1e-6 * np.eye(ld)\n",
    "                invcov = np.linalg.inv(cov)\n",
    "                dM = np.sqrt(((zs - mu_bar) @ invcov * (zs - mu_bar)).sum(axis=1))\n",
    "\n",
    "                # e) Combined sweep\n",
    "                best_a, best_auc = 0, 0\n",
    "                for a in [0, 0.25, 0.5, 0.75, 1]:\n",
    "                    comb = a * errs + (1 - a) * dM\n",
    "                    auc = roc_auc_score(y_val, -comb)\n",
    "                    if auc > best_auc:\n",
    "                        best_a, best_auc = a, auc\n",
    "\n",
    "                # f) XGBoost classifier on validation set\n",
    "                Xf = np.hstack([errs.reshape(-1,1), zs])\n",
    "                clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "                clf.fit(Xf, y_val)\n",
    "                probs = clf.predict_proba(Xf)[:,1]\n",
    "                clf_auc = roc_auc_score(y_val, probs)\n",
    "\n",
    "                results.append({\n",
    "                    'latent_dim': ld,\n",
    "                    'lr': lr,\n",
    "                    'n_blocks': nb,\n",
    "                    'beta': name,\n",
    "                    'best_alpha': best_a,\n",
    "                    'auc_comb': best_auc,\n",
    "                    'auc_xgb': clf_auc\n",
    "                })\n",
    "# Save sweep results\n",
    "pd.DataFrame(results).to_csv('hyperparam_sweep_results.csv', index=False)\n",
    "print(\"Sweep completo. Resultados en hyperparam_sweep_results.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TP-final_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
