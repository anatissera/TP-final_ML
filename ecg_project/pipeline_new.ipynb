{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e246ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d4a9d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dba4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset, Subset, ConcatDataset\n",
    "from pipeline_improved import VAE1D, beta_cyclic, beta_linear, compute_scores, XGBClassifier\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset, ConcatDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "33d44dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from preprocess import load_all_anomalies, compute_zscore_stats, apply_zscore, load_sanos\n",
    "\n",
    "\n",
    "def find_data_subfolder(subfolder_name, start_path='.'):\n",
    "    current_path = os.path.abspath(start_path)\n",
    "    while True:\n",
    "        candidate = os.path.join(current_path, 'data', subfolder_name)\n",
    "        if os.path.isdir(candidate):\n",
    "            return candidate\n",
    "        parent = os.path.dirname(current_path)\n",
    "        if parent == current_path:\n",
    "            break\n",
    "        current_path = parent\n",
    "    return None\n",
    "\n",
    "def find_file(root, filename):\n",
    "    \"\"\"\n",
    "    Busca recursivamente `filename` bajo `root` y devuelve la ruta completa.\n",
    "    \"\"\"\n",
    "    for r, _, files in os.walk(root):\n",
    "        if filename in files:\n",
    "            return os.path.join(r, filename)\n",
    "    return None\n",
    "\n",
    "\n",
    "# Ahora buscás las rutas relativas automáticamente:\n",
    "PTB_DIR = find_data_subfolder('ptb-xl/1.0.3')\n",
    "CHAP_DIR = find_data_subfolder('ChapmanShaoxing')\n",
    "MIT_DIR = find_data_subfolder('mitdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b07b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "normals = load_sanos(PTB_DIR, CHAP_DIR)     \n",
    "ptb_df  = pd.read_csv(find_file(PTB_DIR, 'ptbxl_database.csv')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8fc85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies = load_all_anomalies(PTB_DIR, CHAP_DIR, ptb_df)  # (N_ano,1,L) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "67870d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Split normales en DEV y TEST (80/20)\n",
    "n_norm = normals.shape[0]\n",
    "split_dev = int(0.8 * n_norm)\n",
    "dev_norm  = normals[:split_dev]\n",
    "test_norm = normals[split_dev:]\n",
    "\n",
    "# 3) Dentro de DEV, split en TRAIN y VAL (80/20 of DEV)\n",
    "n_dev = dev_norm.shape[0]\n",
    "split_train = int(0.8 * n_dev)\n",
    "train_norm = dev_norm[:split_train]\n",
    "val_norm   = dev_norm[split_train:]\n",
    "\n",
    "# 4) Normalización Z-score usando solo train_norm) Normalización Z-score usando solo train_norm\n",
    "mean, std = compute_zscore_stats(train_norm)\n",
    "train_norm = apply_zscore(train_norm, mean, std)\n",
    "val_norm   = apply_zscore(val_norm,   mean, std)\n",
    "test_norm  = apply_zscore(test_norm,  mean, std)\n",
    "anomalies  = apply_zscore(anomalies,  mean, std)\n",
    "\n",
    "# 4) Convertir a tensores\n",
    "train_tensor = torch.tensor(train_norm, dtype=torch.float32)\n",
    "val_tensor   = torch.tensor(val_norm,   dtype=torch.float32)\n",
    "test_tensor  = torch.tensor(test_norm,  dtype=torch.float32)\n",
    "ano_tensor   = torch.tensor(anomalies,  dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fbb2d9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros de entrenamiento final\n",
    "latent_dim = 32\n",
    "lr         = 1e-3\n",
    "n_blocks   = 3\n",
    "epochs     = 50\n",
    "batch_size = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61eaef76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 1.2389\n",
      "Epoch 2/50, Loss: 534.0065\n",
      "Epoch 3/50, Loss: 39.3720\n",
      "Epoch 4/50, Loss: 31.5000\n",
      "Epoch 5/50, Loss: 35.4579\n",
      "Epoch 6/50, Loss: 25.8717\n",
      "Epoch 7/50, Loss: 21.8407\n",
      "Epoch 8/50, Loss: 18.6034\n",
      "Epoch 9/50, Loss: 18.6262\n",
      "Epoch 10/50, Loss: 17.6717\n",
      "Epoch 11/50, Loss: 1.0440\n",
      "Epoch 12/50, Loss: 2.5172\n",
      "Epoch 13/50, Loss: 2.0412\n",
      "Epoch 14/50, Loss: 2.1568\n",
      "Epoch 15/50, Loss: 2.3338\n",
      "Epoch 16/50, Loss: 3.1153\n",
      "Epoch 17/50, Loss: 7.5371\n",
      "Epoch 18/50, Loss: 12.1489\n",
      "Epoch 19/50, Loss: 12.9659\n",
      "Epoch 20/50, Loss: 14.2619\n",
      "Epoch 21/50, Loss: 0.9956\n",
      "Epoch 22/50, Loss: 1.7647\n",
      "Epoch 23/50, Loss: 1.6853\n",
      "Epoch 24/50, Loss: 1.4679\n",
      "Epoch 25/50, Loss: 1.4739\n",
      "Epoch 26/50, Loss: 1.5725\n",
      "Epoch 27/50, Loss: 1.7373\n",
      "Epoch 28/50, Loss: 2.6681\n",
      "Epoch 29/50, Loss: 5.3447\n",
      "Epoch 30/50, Loss: 10.8783\n",
      "Epoch 31/50, Loss: 0.9921\n",
      "Epoch 32/50, Loss: 1.7011\n",
      "Epoch 33/50, Loss: 1.5005\n",
      "Epoch 34/50, Loss: 1.2631\n",
      "Epoch 35/50, Loss: 1.1929\n",
      "Epoch 36/50, Loss: 1.1977\n",
      "Epoch 37/50, Loss: 1.2838\n",
      "Epoch 38/50, Loss: 1.6771\n",
      "Epoch 39/50, Loss: 3.8538\n",
      "Epoch 40/50, Loss: 5.7704\n",
      "Epoch 41/50, Loss: 0.9907\n",
      "Epoch 42/50, Loss: 1.1654\n",
      "Epoch 43/50, Loss: 1.1610\n",
      "Epoch 44/50, Loss: 1.0996\n",
      "Epoch 45/50, Loss: 1.0791\n",
      "Epoch 46/50, Loss: 1.0825\n",
      "Epoch 47/50, Loss: 1.0820\n",
      "Epoch 48/50, Loss: 1.1654\n",
      "Epoch 49/50, Loss: 1.5554\n",
      "Epoch 50/50, Loss: 2.4416\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 5) Entrena VAE sobre conjunto DEV (train_norm + val_norm)\n",
    "dev_tensor = torch.cat([train_tensor, val_tensor], dim=0)\n",
    "dev_ds     = TensorDataset(dev_tensor, torch.zeros(len(dev_tensor)))\n",
    "dev_loader = DataLoader(dev_ds, batch_size=batch_size, shuffle=True)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = VAE1D(input_ch=1, latent_dim=latent_dim, n_blocks=n_blocks).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for x, _ in dev_loader:\n",
    "        x = x.to(device)\n",
    "        mu, logv = model.encode(x)\n",
    "        z = model.reparameterize(mu, logv)\n",
    "        rec = model.decode(z)\n",
    "        recon_loss = ((rec - x)**2).mean()\n",
    "        kl_loss = (-0.5 * (1 + logv - mu.pow(2) - logv.exp()).sum()) / x.size(0)\n",
    "        beta = beta_cyclic(epoch, cycle=10, beta_max=4.0)\n",
    "        loss = recon_loss + beta * kl_loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dev_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a38232e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 6) Prepara test final balanceado: test_norm vs primeras anomalías: test_norm vs primeras anomalías\n",
    "N_test = test_tensor.shape[0]\n",
    "test_x = torch.cat([test_tensor, ano_tensor[:N_test]], dim=0)\n",
    "y_test = np.concatenate([np.zeros(N_test), np.ones(N_test)])\n",
    "test_loader = DataLoader(TensorDataset(test_x, torch.tensor(y_test, dtype=torch.long)), batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "51476b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7) Obtiene scores en DEV y TEST por separado ---\n",
    "beta_last = beta_cyclic(epochs-1, cycle=10, beta_max=4.0)\n",
    "\n",
    "# 7a) DEV scores y latentes\n",
    "# -------------------------\n",
    "# Crea DEV loader (train_norm + val_norm)\n",
    "dev_tensor = torch.cat([train_tensor, val_tensor], dim=0)\n",
    "dev_labels = np.concatenate([\n",
    "    np.zeros(len(train_tensor)),        # sanos\n",
    "    np.zeros(len(val_tensor))          # sanos (aquí no hay anomalías)\n",
    "])\n",
    "# (si quieres incluir anomalías de DEV, tendrías que extraer un subset de anomalies)\n",
    "# Para un detector puramente no supervisado podrías omitir el XGBoost en DEV.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57a5a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dev_loader = DataLoader(TensorDataset(dev_tensor, torch.tensor(dev_labels)), batch_size=batch_size)\n",
    "\n",
    "errs_dev, zs_dev = compute_scores(model, dev_loader, device, beta_last)\n",
    "# Limpia de inf/nan\n",
    "errs_dev = np.nan_to_num(errs_dev, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "zs_dev   = np.nan_to_num(zs_dev,   nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "\n",
    "X_dev = np.hstack([errs_dev.reshape(-1,1), zs_dev])\n",
    "y_dev = dev_labels  # etiquetas de DEV\n",
    "\n",
    "# Entrena XGBoost en DEV\n",
    "clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "clf.fit(X_dev, y_dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16886335",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 7b) TEST scores y latentes ---\n",
    "# -------------------------\n",
    "errs_test, zs_test = compute_scores(model, test_loader, device, beta_last)\n",
    "errs_test = np.nan_to_num(errs_test, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "zs_test   = np.nan_to_num(zs_test,   nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "\n",
    "X_test = np.hstack([errs_test.reshape(-1,1), zs_test])\n",
    "y_test  = np.concatenate([np.zeros(len(test_tensor)), np.ones(len(ano_tensor[:len(test_tensor)]))])\n",
    "\n",
    "# Solo inferencia sobre TEST\n",
    "probs = clf.predict_proba(X_test)[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9fba4b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, accuracy_score, r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5048bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final metrics: {'roc_auc': np.float64(1.0), 'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'accuracy': 1.0, 'r2': 0.9999610804432385}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 10) Métricas finales en TEST ---\n",
    "auc      = roc_auc_score(y_test, probs)\n",
    "y_pred   = (probs >= 0.5).astype(int)\n",
    "metrics  = {\n",
    "    'roc_auc':   auc,\n",
    "    'precision': precision_score(y_test, y_pred),\n",
    "    'recall':    recall_score(y_test, y_pred),\n",
    "    'f1':        f1_score(y_test, y_pred),\n",
    "    'accuracy':  accuracy_score(y_test, y_pred),\n",
    "    'r2':        r2_score(y_test, probs)\n",
    "}\n",
    "print('Final metrics on TEST (unseen):', metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ee67a945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 1.1958\n",
      "Epoch 2/50, Loss: 202.6760\n",
      "Epoch 3/50, Loss: 16.5132\n",
      "Epoch 4/50, Loss: 12.4264\n",
      "Epoch 5/50, Loss: 9.1345\n",
      "Epoch 6/50, Loss: 8.9151\n",
      "Epoch 7/50, Loss: 8.4336\n",
      "Epoch 8/50, Loss: 7.5735\n",
      "Epoch 9/50, Loss: 7.4693\n",
      "Epoch 10/50, Loss: 7.1344\n",
      "Epoch 11/50, Loss: 1.0399\n",
      "Epoch 12/50, Loss: 2.1132\n",
      "Epoch 13/50, Loss: 1.5026\n",
      "Epoch 14/50, Loss: 1.3857\n",
      "Epoch 15/50, Loss: 1.4055\n",
      "Epoch 16/50, Loss: 1.5078\n",
      "Epoch 17/50, Loss: 2.4385\n",
      "Epoch 18/50, Loss: 4.3530\n",
      "Epoch 19/50, Loss: 9.3779\n",
      "Epoch 20/50, Loss: 7.1084\n",
      "Epoch 21/50, Loss: 0.9955\n",
      "Epoch 22/50, Loss: 1.3916\n",
      "Epoch 23/50, Loss: 1.3126\n",
      "Epoch 24/50, Loss: 1.2037\n",
      "Epoch 25/50, Loss: 1.1607\n",
      "Epoch 26/50, Loss: 1.1495\n",
      "Epoch 27/50, Loss: 1.1525\n",
      "Epoch 28/50, Loss: 1.2128\n",
      "Epoch 29/50, Loss: 1.4502\n",
      "Epoch 30/50, Loss: 2.4173\n",
      "Epoch 31/50, Loss: 0.9905\n",
      "Epoch 32/50, Loss: 1.2744\n",
      "Epoch 33/50, Loss: 1.1224\n",
      "Epoch 34/50, Loss: 1.0621\n",
      "Epoch 35/50, Loss: 1.0562\n",
      "Epoch 36/50, Loss: 1.0880\n",
      "Epoch 37/50, Loss: 1.4443\n",
      "Epoch 38/50, Loss: 2.8737\n",
      "Epoch 39/50, Loss: 2.6032\n",
      "Epoch 40/50, Loss: 2.2055\n",
      "Epoch 41/50, Loss: 0.9886\n",
      "Epoch 42/50, Loss: 1.0256\n",
      "Epoch 43/50, Loss: 1.0352\n",
      "Epoch 44/50, Loss: 1.0249\n",
      "Epoch 45/50, Loss: 1.0183\n",
      "Epoch 46/50, Loss: 1.0150\n",
      "Epoch 47/50, Loss: 1.0120\n",
      "Epoch 48/50, Loss: 1.0162\n",
      "Epoch 49/50, Loss: 1.0609\n",
      "Epoch 50/50, Loss: 1.1719\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 5) Entrena VAE sobre conjunto DEV (train_norm + val_norm)\n",
    "dev_tensor = torch.cat([train_tensor, val_tensor], dim=0)\n",
    "dev_ds     = TensorDataset(dev_tensor, torch.zeros(len(dev_tensor)))\n",
    "dev_loader = DataLoader(dev_ds, batch_size=batch_size, shuffle=True)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = VAE1D(input_ch=1, latent_dim=latent_dim, n_blocks=n_blocks).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for x, _ in dev_loader:\n",
    "        x = x.to(device)\n",
    "        mu, logv = model.encode(x)\n",
    "        z = model.reparameterize(mu, logv)\n",
    "        rec = model.decode(z)\n",
    "        recon_loss = ((rec - x)**2).mean()\n",
    "        kl_loss = (-0.5 * (1 + logv - mu.pow(2) - logv.exp()).sum()) / x.size(0)\n",
    "        beta = beta_cyclic(epoch, cycle=10, beta_max=4.0)\n",
    "        loss = recon_loss + beta * kl_loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dev_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2264ecd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 6) Split anomalies into DEV and TEST (80/20)\n",
    "n_ano = anomalies.shape[0]\n",
    "split_ano = int(0.8 * n_ano)\n",
    "ano_dev  = anomalies[:split_ano]\n",
    "ano_test = anomalies[split_ano:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e24d3c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ELBO ===\n",
      "AUC      : 0.548\n",
      "Precision: 0.637\n",
      "Recall   : 0.278\n",
      "F1       : 0.387\n",
      "Accuracy : 0.560\n",
      "R2       : -98.093\n",
      "\n",
      "=== Mahalanobis ===\n",
      "AUC      : 0.586\n",
      "Precision: 0.614\n",
      "Recall   : 0.366\n",
      "F1       : 0.458\n",
      "Accuracy : 0.568\n",
      "R2       : -147.233\n",
      "\n",
      "=== IsolationForest ===\n",
      "AUC      : 0.562\n",
      "Precision: 0.624\n",
      "Recall   : 0.279\n",
      "F1       : 0.386\n",
      "Accuracy : 0.555\n",
      "R2       : -1.000\n",
      "\n",
      "=== OC-SVM ===\n",
      "AUC      : 0.561\n",
      "Precision: 0.629\n",
      "Recall   : 0.265\n",
      "F1       : 0.373\n",
      "Accuracy : 0.554\n",
      "R2       : -1181.057\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, precision_score, recall_score,\n",
    "    f1_score, accuracy_score, r2_score, roc_curve\n",
    ")\n",
    "from sklearn.covariance import EmpiricalCovariance\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "# --- 7) Obtener scores ELBO y latentes en TEST final ---\n",
    "beta_last = beta_cyclic(epochs-1, cycle=10, beta_max=4.0)\n",
    "errs_test, zs_test = compute_scores(model, test_loader, device, beta_last)\n",
    "\n",
    "# Limpiar inf/nan\n",
    "maxv = np.finfo(np.float32).max/10\n",
    "minv = np.finfo(np.float32).min/10\n",
    "errs_test = np.nan_to_num(errs_test, nan=0.0, posinf=maxv, neginf=minv)\n",
    "zs_test   = np.nan_to_num(zs_test,   nan=0.0, posinf=maxv, neginf=minv)\n",
    "\n",
    "# Etiquetas reales de TEST\n",
    "# test_norm y ano_test definidos previamente\n",
    "N_test = test_norm.shape[0]\n",
    "y_test = np.concatenate([np.zeros(N_test), np.ones(N_test)])\n",
    "\n",
    "# --- 8) Mahalanobis en espacio latente ---\n",
    "# Ajustamos la covarianza solo con z de normales de DEV (train+val)\n",
    "# dev_normals: numpy array (N_train+N_val,1,L)\n",
    "dev_normals = np.concatenate([train_norm, val_norm], axis=0)\n",
    "\n",
    "# Para obtener z_train_norm\n",
    "errs_dev_norm, zs_dev_norm = compute_scores(\n",
    "    model,\n",
    "    DataLoader(\n",
    "        TensorDataset(\n",
    "            torch.tensor(dev_normals, dtype=torch.float32),\n",
    "            torch.zeros(len(dev_normals))  # labels zeros, no importan aquí\n",
    "        ),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    ),\n",
    "    device,\n",
    "    beta_last\n",
    ")\n",
    "\n",
    "zs_dev_norm = np.nan_to_num(zs_dev_norm, nan=0.0, posinf=maxv, neginf=minv)\n",
    "cov_est = EmpiricalCovariance().fit(zs_dev_norm)\n",
    "# distancia Mahalanobis para TEST\n",
    "dM = cov_est.mahalanobis(zs_test)\n",
    "dM = np.sqrt(dM)\n",
    "\n",
    "# --- 9) Detectores no supervisados y thresholding ---\n",
    "\n",
    "results = {}\n",
    "\n",
    "# 9a) ELBO-threshold\n",
    "probs_elbo = -errs_test\n",
    "fpr, tpr, thr = roc_curve(y_test, probs_elbo)\n",
    "youden = np.argmax(tpr - fpr)\n",
    "thr_elbo = thr[youden]\n",
    "y_pred_elbo = (probs_elbo >= thr_elbo).astype(int)\n",
    "\n",
    "\n",
    "results['ELBO'] = {\n",
    "    'auc': roc_auc_score(y_test, probs_elbo),\n",
    "    'precision': precision_score(y_test, y_pred_elbo),\n",
    "    'recall':    recall_score(y_test, y_pred_elbo),\n",
    "    'f1':        f1_score(y_test, y_pred_elbo),\n",
    "    'accuracy':  accuracy_score(y_test, y_pred_elbo),\n",
    "    'r2':        r2_score(y_test, probs_elbo)\n",
    "}\n",
    "\n",
    "# 9b) Mahalanobis-threshold\n",
    "probs_maha = dM\n",
    "fpr2, tpr2, thr2 = roc_curve(y_test, probs_maha)\n",
    "youden2 = np.argmax(tpr2 - fpr2)\n",
    "thr_maha = thr2[youden2]\n",
    "y_pred_maha = (probs_maha >= thr_maha).astype(int)\n",
    "results['Mahalanobis'] = {\n",
    "    'auc': roc_auc_score(y_test, probs_maha),\n",
    "    'precision': precision_score(y_test, y_pred_maha),\n",
    "    'recall':    recall_score(y_test, y_pred_maha),\n",
    "    'f1':        f1_score(y_test, y_pred_maha),\n",
    "    'accuracy':  accuracy_score(y_test, y_pred_maha),\n",
    "    'r2':        r2_score(y_test, probs_maha)\n",
    "}\n",
    "\n",
    "# 9c) Isolation Forest sobre z\n",
    "iso = IsolationForest(contamination=N_test/len(zs_dev_norm), random_state=0)\n",
    "iso.fit(zs_dev_norm)\n",
    "scores_iso = -iso.decision_function(zs_test)\n",
    "fpr3, tpr3, thr3 = roc_curve(y_test, scores_iso)\n",
    "youden3 = np.argmax(tpr3 - fpr3)\n",
    "thr_iso = thr3[youden3]\n",
    "y_pred_iso = (scores_iso >= thr_iso).astype(int)\n",
    "results['IsolationForest'] = {\n",
    "    'auc': roc_auc_score(y_test, scores_iso),\n",
    "    'precision': precision_score(y_test, y_pred_iso),\n",
    "    'recall':    recall_score(y_test, y_pred_iso),\n",
    "    'f1':        f1_score(y_test, y_pred_iso),\n",
    "    'accuracy':  accuracy_score(y_test, y_pred_iso),\n",
    "    'r2':        r2_score(y_test, scores_iso)\n",
    "}\n",
    "\n",
    "# 9d) One-Class SVM sobre z\n",
    "ocsvm = OneClassSVM(gamma='auto', nu=0.05)\n",
    "ocsvm.fit(zs_dev_norm)\n",
    "scores_oc = -ocsvm.decision_function(zs_test)\n",
    "fpr4, tpr4, thr4 = roc_curve(y_test, scores_oc)\n",
    "youden4 = np.argmax(tpr4 - fpr4)\n",
    "thr_oc = thr4[youden4]\n",
    "y_pred_oc = (scores_oc >= thr_oc).astype(int)\n",
    "results['OC-SVM'] = {\n",
    "    'auc': roc_auc_score(y_test, scores_oc),\n",
    "    'precision': precision_score(y_test, y_pred_oc),\n",
    "    'recall':    recall_score(y_test, y_pred_oc),\n",
    "    'f1':        f1_score(y_test, y_pred_oc),\n",
    "    'accuracy':  accuracy_score(y_test, y_pred_oc),\n",
    "    'r2':        r2_score(y_test, scores_oc)\n",
    "}\n",
    "\n",
    "# --- 10) Mostrar todos los resultados ---\n",
    "for name, m in results.items():\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(f\"AUC      : {m['auc']:.3f}\")\n",
    "    print(f\"Precision: {m['precision']:.3f}\")\n",
    "    print(f\"Recall   : {m['recall']:.3f}\")\n",
    "    print(f\"F1       : {m['f1']:.3f}\")\n",
    "    print(f\"Accuracy : {m['accuracy']:.3f}\")\n",
    "    print(f\"R2       : {m['r2']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ca39f620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 1.2378\n",
      "Epoch 2/50, Loss: 1146.0367\n",
      "Epoch 3/50, Loss: 43.5596\n",
      "Epoch 4/50, Loss: 29.1880\n",
      "Epoch 5/50, Loss: 34.6443\n",
      "Epoch 6/50, Loss: 31.6987\n",
      "Epoch 7/50, Loss: 26.9225\n",
      "Epoch 8/50, Loss: 22.3716\n",
      "Epoch 9/50, Loss: 19.0170\n",
      "Epoch 10/50, Loss: 20.0865\n",
      "Epoch 11/50, Loss: 1.1884\n",
      "Epoch 12/50, Loss: 3.2446\n",
      "Epoch 13/50, Loss: 2.7533\n",
      "Epoch 14/50, Loss: 3.0681\n",
      "Epoch 15/50, Loss: 3.4334\n",
      "Epoch 16/50, Loss: 4.3026\n",
      "Epoch 17/50, Loss: 5.7991\n",
      "Epoch 18/50, Loss: 17.3754\n",
      "Epoch 19/50, Loss: 18.6135\n",
      "Epoch 20/50, Loss: 18.5862\n",
      "Epoch 21/50, Loss: 1.0847\n",
      "Epoch 22/50, Loss: 2.7654\n",
      "Epoch 23/50, Loss: 2.2581\n",
      "Epoch 24/50, Loss: 1.9074\n",
      "Epoch 25/50, Loss: 1.8397\n",
      "Epoch 26/50, Loss: 1.9659\n",
      "Epoch 27/50, Loss: 2.1030\n",
      "Epoch 28/50, Loss: 3.8655\n",
      "Epoch 29/50, Loss: 8.3550\n",
      "Epoch 30/50, Loss: 13.0368\n",
      "Epoch 31/50, Loss: 1.0300\n",
      "Epoch 32/50, Loss: 2.1466\n",
      "Epoch 33/50, Loss: 1.7997\n",
      "Epoch 34/50, Loss: 1.4958\n",
      "Epoch 35/50, Loss: 1.4171\n",
      "Epoch 36/50, Loss: 1.5791\n",
      "Epoch 37/50, Loss: 3.4036\n",
      "Epoch 38/50, Loss: 8.4945\n",
      "Epoch 39/50, Loss: 8.5200\n",
      "Epoch 40/50, Loss: 10.7587\n",
      "Epoch 41/50, Loss: 1.0004\n",
      "Epoch 42/50, Loss: 1.5780\n",
      "Epoch 43/50, Loss: 1.4386\n",
      "Epoch 44/50, Loss: 1.2741\n",
      "Epoch 45/50, Loss: 1.2073\n",
      "Epoch 46/50, Loss: 1.1937\n",
      "Epoch 47/50, Loss: 1.1762\n",
      "Epoch 48/50, Loss: 1.2848\n",
      "Epoch 49/50, Loss: 1.5969\n",
      "Epoch 50/50, Loss: 3.4094\n",
      "Final metrics on unseen TEST: {'roc_auc': np.float64(0.5726220098309802), 'precision': 0.5, 'recall': 0.9905412506568576, 'f1': 0.6645513837475763, 'accuracy': 0.5, 'r2': -0.2865621468655559}\n"
     ]
    }
   ],
   "source": [
    "# 2) Split normales en DEV y TEST (80/20)\n",
    "n_norm = normals.shape[0]\n",
    "split_dev = int(0.8 * n_norm)\n",
    "dev_norm  = normals[:split_dev]\n",
    "test_norm = normals[split_dev:]\n",
    "\n",
    "# 3) Dentro de DEV, split en TRAIN y VAL (80/20 of DEV)\n",
    "n_dev = dev_norm.shape[0]\n",
    "split_train = int(0.8 * n_dev)\n",
    "train_norm = dev_norm[:split_train]\n",
    "val_norm   = dev_norm[split_train:]\n",
    "\n",
    "# 4) Normalización Z-score usando solo train_norm) Normalización Z-score usando solo train_norm\n",
    "mean, std = compute_zscore_stats(train_norm)\n",
    "train_norm = apply_zscore(train_norm, mean, std)\n",
    "val_norm   = apply_zscore(val_norm,   mean, std)\n",
    "test_norm  = apply_zscore(test_norm,  mean, std)\n",
    "anomalies  = apply_zscore(anomalies,  mean, std)\n",
    "\n",
    "# 4) Convertir a tensores\n",
    "train_tensor = torch.tensor(train_norm, dtype=torch.float32)\n",
    "val_tensor   = torch.tensor(val_norm,   dtype=torch.float32)\n",
    "test_tensor  = torch.tensor(test_norm,  dtype=torch.float32)\n",
    "ano_tensor   = torch.tensor(anomalies,  dtype=torch.float32)\n",
    "\n",
    "# Parámetros de entrenamiento final\n",
    "latent_dim = 32\n",
    "lr         = 1e-3\n",
    "n_blocks   = 3\n",
    "epochs     = 50\n",
    "batch_size = 32\n",
    "\n",
    "# 5) Entrena VAE sobre conjunto DEV (train_norm + val_norm)\n",
    "dev_tensor = torch.cat([train_tensor, val_tensor], dim=0)\n",
    "dev_ds     = TensorDataset(dev_tensor, torch.zeros(len(dev_tensor)))\n",
    "dev_loader = DataLoader(dev_ds, batch_size=batch_size, shuffle=True)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = VAE1D(input_ch=1, latent_dim=latent_dim, n_blocks=n_blocks).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for x, _ in dev_loader:\n",
    "        x = x.to(device)\n",
    "        mu, logv = model.encode(x)\n",
    "        z = model.reparameterize(mu, logv)\n",
    "        rec = model.decode(z)\n",
    "        recon_loss = ((rec - x)**2).mean()\n",
    "        kl_loss = (-0.5 * (1 + logv - mu.pow(2) - logv.exp()).sum()) / x.size(0)\n",
    "        beta = beta_cyclic(epoch, cycle=10, beta_max=4.0)\n",
    "        loss = recon_loss + beta * kl_loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dev_loader):.4f}\")\n",
    "\n",
    "    # 6) Split anomalies into DEV and TEST (80/20)\n",
    "n_ano = anomalies.shape[0]\n",
    "split_ano = int(0.8 * n_ano)\n",
    "ano_dev  = anomalies[:split_ano]\n",
    "ano_test = anomalies[split_ano:]\n",
    "\n",
    "# 7) Entrena XGBoost y evalúa:\n",
    "#   a) Genera scores ELBO y z para DEV (normales DEV + anomalías DEV)\n",
    "# DEV normales ya en dev_tensor (train+val), dev_normals_labels = zeros\n",
    "dev_normals = np.concatenate([train_norm, val_norm], axis=0)\n",
    "dev_labels_norm = np.zeros(dev_normals.shape[0])\n",
    "# DEV anomalies\n",
    "dev_labels_ano = np.ones(ano_dev.shape[0])\n",
    "# Prepara loader para DEV classifier scoring\n",
    "dev_x = np.concatenate([dev_normals, ano_dev], axis=0)\n",
    "dev_y = np.concatenate([dev_labels_norm, dev_labels_ano])\n",
    "# TensorDataset expects torch tensors\n",
    "dev_loader = DataLoader(\n",
    "    TensorDataset(\n",
    "        torch.tensor(dev_x, dtype=torch.float32),\n",
    "        torch.tensor(dev_y, dtype=torch.long)\n",
    "    ), batch_size=batch_size, shuffle=False\n",
    ")\n",
    "errs_dev, zs_dev = compute_scores(model, dev_loader, device, beta_last)\n",
    "errs_dev = np.nan_to_num(errs_dev, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "zs_dev   = np.nan_to_num(zs_dev,   nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "X_dev = np.hstack([errs_dev.reshape(-1,1), zs_dev])\n",
    "y_dev = dev_y\n",
    "\n",
    "# Entrena XGBoost en DEV (ahora con dos clases)\n",
    "clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "clf.fit(X_dev, y_dev)\n",
    "\n",
    "# 8) Evalúa en TEST final (normales TEST + anomalías TEST)\n",
    "N_test = test_norm.shape[0]\n",
    "test_x = np.concatenate([test_norm, ano_test[:N_test]], axis=0)\n",
    "test_y = np.concatenate([np.zeros(N_test), np.ones(N_test)])\n",
    "# Scores TEST\n",
    "test_loader = DataLoader(\n",
    "    TensorDataset(\n",
    "        torch.tensor(test_x, dtype=torch.float32),\n",
    "        torch.tensor(test_y, dtype=torch.long)\n",
    "    ), batch_size=batch_size, shuffle=False\n",
    ")\n",
    "errs_test, zs_test = compute_scores(model, test_loader, device, beta_last)\n",
    "errs_test = np.nan_to_num(errs_test, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "zs_test   = np.nan_to_num(zs_test,   nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "X_test = np.hstack([errs_test.reshape(-1,1), zs_test])\n",
    "y_test = test_y\n",
    "# Predicciones\n",
    "probs = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "# 9) Métricas finales en TEST\n",
    "auc      = roc_auc_score(y_test, probs)\n",
    "y_pred   = (probs >= 0.5).astype(int)\n",
    "metrics  = {\n",
    "    'roc_auc':   auc,\n",
    "    'precision': precision_score(y_test, y_pred),\n",
    "    'recall':    recall_score(y_test, y_pred),\n",
    "    'f1':        f1_score(y_test, y_pred),\n",
    "    'accuracy':  accuracy_score(y_test, y_pred),\n",
    "    'r2':        r2_score(y_test, probs)\n",
    "}\n",
    "print('Final metrics on unseen TEST:', metrics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TP-final_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
