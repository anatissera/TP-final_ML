--- PROJECT STRUCTURE (`tree -L 3 -a -I ...`) ---
```text
ERROR: 'tree' command not found. Please install it (e.g., 'brew install tree' or 'sudo apt-get install tree').
```

========================================


--- START FILE: ecg_data_exploration_raw.py ---

```python
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.signal import butter, filtfilt
from scipy.fft import fft, fftfreq
import wfdb
from wfdb import rdrecord
from scipy.io import loadmat

# ---------- Utilities ----------
def list_directory(start_dir, depth=2):
    """
    Prints directory structure up to given depth.
    """
    for root, dirs, files in os.walk(start_dir):
        level = root.replace(start_dir, '').count(os.sep)
        if level > depth:
            continue
        indent = '  ' * level
        # print(f"{indent}{os.path.basename(root)}/")
        # if level < depth:
        #     for f in files:
        #         print(f"{indent}  - {f}")

# ---------- Parse Chapman Header ----------
def parse_chapman_header(hea_path):
    """
    Extracts metadata from Chapman-Shaoxing .hea header.
    """
    with open(hea_path, 'r') as f:
        lines = [l.strip() for l in f.readlines()]
    parts = lines[0].split()
    record_id = parts[0].replace('.mat', '')
    age = next((l.split()[1] for l in lines if l.startswith('#Age:')), None)
    sex = next((l.split()[1] for l in lines if l.startswith('#Sex:')), None)
    dx  = next((l.split()[1] for l in lines if l.startswith('#Dx:')), None)
    return {'record': record_id, 'age': age, 'sex': sex, 'diagnosis': dx}

# ---------- Loading Metadata ----------

def load_ptbxl_metadata(ptb_dir):
    csv = next((os.path.join(r, f) for r, _, fs in os.walk(ptb_dir)
                for f in fs if f == 'ptbxl_database.csv'), None)
    df = pd.read_csv(csv)
    return df[['ecg_id', 'filename_lr', 'scp_codes', 'age', 'sex']]

def load_chapman_metadata(path):
    recs = []
    for root, _, files in os.walk(path):
        for file in files:
            if file.endswith('.hea') and 'Zone' not in file:
                recs.append(parse_chapman_header(os.path.join(root, file)))
    return pd.DataFrame(recs)

def filter_chapman_normal(df):
    return df[df['diagnosis'].str.contains('426177001', na=False)].reset_index(drop=True)

# ---------- Signal Loading ----------

def load_ptbxl_signal(ptb_dir, filename_lr):
    """
    Loads PTB-XL record by locating the .hea header matching filename_lr.
    filename_lr is metadata string like 'records100/00000/00001_lr'.
    """
    # Derive expected header basename
    hea_basename = os.path.basename(filename_lr) + '.hea'
    hea_path = None
    for root, _, files in os.walk(ptb_dir):
        if hea_basename in files:
            hea_path = os.path.join(root, hea_basename)
            break
    if hea_path is None:
        raise FileNotFoundError(f"No se encontró header PTB-XL para '{filename_lr}' (buscando '{hea_basename}') en {ptb_dir}")
    # Record prefix is full path without extension
    record_prefix = os.path.splitext(hea_path)[0]
    record = rdrecord(record_prefix)
    return record.p_signal.T, record.fs

def load_chapman_signal(chap_dir, record_id):
    mat_path = os.path.join(chap_dir, f"{record_id}.mat")
    data = loadmat(mat_path)['val']
    return data, 500

# ---------- Visualization ----------

def plot_signal(time, signal, lead=0, title="ECG Signal"):
    plt.figure()
    plt.plot(time, signal[lead])
    plt.xlabel('Time (s)')
    plt.ylabel('Amplitude')
    plt.title(f"{title} - Lead {lead}")
    plt.grid(True)
    plt.show()

def plot_fft(signal, fs, lead=0, title="FFT of ECG Signal"):
    n = signal.shape[1]
    yf = fft(signal[lead])
    xf = fftfreq(n, 1 / fs)
    plt.figure()
    plt.plot(xf[:n//2], np.abs(yf[:n//2]))
    plt.xlabel('Frequency (Hz)')
    plt.ylabel('Magnitude')
    plt.title(f"{title} - Lead {lead}")
    plt.grid(True)
    plt.show()

# ---------- Filtering ----------

def butter_lowpass(cutoff, fs, order=4):
    nyq = 0.5 * fs
    normal_cutoff = cutoff / nyq
    return butter(order, normal_cutoff, btype='low', analog=False)

def apply_lowpass(data, fs, cutoff=40.0, order=4):
    b, a = butter_lowpass(cutoff, fs, order)
    return filtfilt(b, a, data)

# ---------- Main Analysis ----------

if __name__ == '__main__':
    PTBXL_DIR = 'data/ptb-xl'
    CHAPMAN_DIR = 'data/ChapmanShaoxing'

    # Directory trees
    print("PTB-XL structure:")
    list_directory(PTBXL_DIR, depth=3)
    print("\nChapman-Shaoxing structure:")
    list_directory(CHAPMAN_DIR, depth=2)

    # Metadata
    ptb_meta = load_ptbxl_metadata(PTBXL_DIR)
    ptb_norm = ptb_meta[ptb_meta['scp_codes'].str.contains('NORM', na=False)]
    print(f"PTB-XL normales: {len(ptb_norm)} registros")

    chap_meta = load_chapman_metadata(CHAPMAN_DIR)
    chap_norm = filter_chapman_normal(chap_meta)
    print(f"Chapman-Shaoxing normales: {len(chap_norm)} registros")

    # PTB-XL sample
    sample = ptb_norm.iloc[0]
    sig, fs = load_ptbxl_signal(PTBXL_DIR, sample['filename_lr'])
    time = np.arange(sig.shape[1]) / fs
    plot_signal(time, sig, title="Raw PTB-XL ECG")

    # Chapman sample
    if len(chap_norm):
        rec = chap_norm.iloc[0]['record']
        chap_sig, chap_fs = load_chapman_signal(CHAPMAN_DIR, rec)
        chap_time = np.arange(chap_sig.shape[1]) / chap_fs
        plot_signal(chap_time, chap_sig, title="Raw Chapman ECG")
    else:
        print("No hay registros normales de Chapman para plotear.")
```

--- END FILE: ecg_data_exploration_raw.py ---

--- START FILE: ecg_data_exploration_scaled_filtered.py ---

```python
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.signal import butter, filtfilt
import wfdb
from wfdb import rdrecord
from scipy.io import loadmat

# ---------- Utilities ----------
def list_directory(start_dir, depth=2):
    """Prints directory structure up to given depth."""
    for root, dirs, files in os.walk(start_dir):
        level = root.replace(start_dir, '').count(os.sep)
        if level > depth:
            continue
        indent = '  ' * level
        # print(f"{indent}{os.path.basename(root)}/")
        # if level < depth:
        #     for f in files:
        #         print(f"{indent}  - {f}")

# ---------- Parse Chapman Header ----------
def parse_chapman_header(hea_path):
    with open(hea_path, 'r') as f:
        lines = [l.strip() for l in f.readlines()]
    parts = lines[0].split()
    record_id = parts[0].replace('.mat', '')
    age = next((l.split()[1] for l in lines if l.startswith('#Age:')), None)
    sex = next((l.split()[1] for l in lines if l.startswith('#Sex:')), None)
    dx  = next((l.split()[1] for l in lines if l.startswith('#Dx:')), None)
    return {'record': record_id, 'age': age, 'sex': sex, 'diagnosis': dx}

# ---------- Loading Metadata ----------
def load_ptbxl_metadata(ptb_dir):
    csv = next((os.path.join(r, f) for r, _, fs in os.walk(ptb_dir)
                for f in fs if f == 'ptbxl_database.csv'), None)
    df = pd.read_csv(csv)
    return df[['ecg_id', 'filename_lr', 'scp_codes', 'age', 'sex']]

def load_chapman_metadata(path):
    recs = []
    for root, _, files in os.walk(path):
        for file in files:
            if file.endswith('.hea') and 'Zone' not in file:
                recs.append(parse_chapman_header(os.path.join(root, file)))
    return pd.DataFrame(recs)

def filter_chapman_normal(df):
    return df[df['diagnosis'].str.contains('426177001', na=False)].reset_index(drop=True)

# ---------- Signal Loading ----------
def load_ptbxl_signal(ptb_dir, filename_lr):
    hea_basename = os.path.basename(filename_lr) + '.hea'
    hea_path = None
    for root, _, files in os.walk(ptb_dir):
        if hea_basename in files:
            hea_path = os.path.join(root, hea_basename)
            break
    if hea_path is None:
        raise FileNotFoundError(f"No se encontró header PTB-XL para '{filename_lr}' (buscando '{hea_basename}') en {ptb_dir}")
    record_prefix = os.path.splitext(hea_path)[0]
    record = rdrecord(record_prefix)
    return record.p_signal.T, record.fs

def load_chapman_signal(chap_dir, record_id):
    mat_path = os.path.join(chap_dir, f"{record_id}.mat")
    data = loadmat(mat_path)['val']
    return data, 500

# ---------- Preprocessing ----------
def butter_bandpass(lowcut, highcut, fs, order=4):
    nyq = 0.5 * fs
    low = lowcut / nyq
    high = highcut / nyq
    b, a = butter(order, [low, high], btype='band')
    return b, a

def apply_bandpass(data, fs, lowcut=0.5, highcut=40.0, order=4):
    b, a = butter_bandpass(lowcut, highcut, fs, order)
    return filtfilt(b, a, data)

def normalize_signal(data, method='zscore'):
    """Normalize each channel: 'zscore' or 'minmax'."""
    if method == 'zscore':
        mu = np.mean(data, axis=1, keepdims=True)
        sigma = np.std(data, axis=1, keepdims=True)
        return (data - mu) / (sigma + 1e-8)
    elif method == 'minmax':
        minv = np.min(data, axis=1, keepdims=True)
        maxv = np.max(data, axis=1, keepdims=True)
        return 2 * (data - minv) / (maxv - minv + 1e-8) - 1
    else:
        raise ValueError(f"Método de normalización desconocido: {method}")

# ---------- Visualization ----------
def plot_signal(time, signal, lead=0, title="ECG Signal"):
    plt.figure()
    plt.plot(time, signal[lead])
    plt.xlabel('Time (s)')
    plt.ylabel('Amplitude')
    plt.title(f"{title} - Lead {lead}")
    plt.grid(True)
    plt.show()

# ---------- Main Analysis ----------
if __name__ == '__main__':
    PTBXL_DIR = 'data/ptb-xl'
    CHAPMAN_DIR = 'data/ChapmanShaoxing'

    print("PTB-XL structure:")
    list_directory(PTBXL_DIR, depth=3)
    print("\nChapman-Shaoxing structure:")
    list_directory(CHAPMAN_DIR, depth=2)

    # Load metadata and filter normals
    ptb_meta = load_ptbxl_metadata(PTBXL_DIR)
    ptb_norm = ptb_meta[ptb_meta['scp_codes'].str.contains('NORM', na=False)]
    print(f"PTB-XL normales: {len(ptb_norm)} registros")

    chap_meta = load_chapman_metadata(CHAPMAN_DIR)
    chap_norm = filter_chapman_normal(chap_meta)
    print(f"Chapman-Shaoxing normales: {len(chap_norm)} registros")

    # PTB-XL sample
    sample = ptb_norm.iloc[0]
    sig_ptb, fs_ptb = load_ptbxl_signal(PTBXL_DIR, sample['filename_lr'])
    sig_ptb = apply_bandpass(sig_ptb, fs_ptb)
    sig_ptb = normalize_signal(sig_ptb, method='minmax')  # escala [-1,1]
    time_ptb = np.arange(sig_ptb.shape[1]) / fs_ptb
    plot_signal(time_ptb, sig_ptb, title="Preprocessed PTB-XL ECG [-1,1]")

    # Chapman sample
    if len(chap_norm) > 0:
        rec = chap_norm.iloc[0]['record']
        sig_chap, fs_chap = load_chapman_signal(CHAPMAN_DIR, rec)
        sig_chap = apply_bandpass(sig_chap, fs_chap)
        sig_chap = normalize_signal(sig_chap, method='minmax')  # escala [-1,1]
        time_chap = np.arange(sig_chap.shape[1]) / fs_chap
        plot_signal(time_chap, sig_chap, title="Preprocessed Chapman ECG [-1,1]")
    else:
        print("No hay registros normales de Chapman para procesar.")
```

--- END FILE: ecg_data_exploration_scaled_filtered.py ---

--- START FILE: ecg_preprocessing_analysis.py ---

```python
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.signal import butter, filtfilt
from scipy.fft import fft, fftfreq
import wfdb
from wfdb import rdrecord
from scipy.io import loadmat

# ---------- Utilities ----------
def list_directory(start_dir, depth=1):
    """
    Prints directory structure up to given depth.
    """
    for root, dirs, files in os.walk(start_dir):
        level = root.replace(start_dir, '').count(os.sep)
        if level > depth:
            continue
        indent = '  ' * level
        print(f"{indent}{os.path.basename(root)}/")
        if level < depth:
            for f in files:
                print(f"{indent}  - {f}")

# ---------- Parse Chapman Header ----------
def parse_chapman_header(hea_path):
    """
    Extracts metadata from Chapman-Shaoxing .hea header.
    Returns dict with record id, fs, samples, age, sex, diagnosis, and lead boundaries.
    """
    with open(hea_path, 'r') as f:
        lines = [l.strip() for l in f.readlines()]
    # Header line example: JS00001.mat 16+24 1000/mV 16 0 -254 21756 0 I
    parts = lines[0].split()
    record_id = parts[0].replace('.mat', '')
    n_leads = parts[1]
    fs = parts[2]
    total_samples = parts[3]
    # Following 12 lines: lead info
    leads = {}
    for i in range(1, 13):
        row = lines[i].split()
        first_val = row[5]
        last_val = row[6]
        lead_name = row[7]
        leads[lead_name] = {'first': first_val, 'last': last_val}
    # Clinical metadata at end
    # Last line format: Aged=XX Sex=M dx=426177001 ...
    meta_line = next((l for l in lines if 'dx=' in l), '')
    age = None
    sex = None
    dx = None
    for token in meta_line.split():
        if token.startswith('Age=') or token.startswith('Age='):
            age = token.split('=')[1]
        if token.startswith('Sex='):
            sex = token.split('=')[1]
        if token.startswith('dx='):
            dx = token.split('=')[1]
    metadata = {
        'record': record_id,
        'n_leads': n_leads,
        'fs': fs,
        'samples': total_samples,
        'age': age,
        'sex': sex,
        'diagnosis': dx,
    }
    # Include lead first/last values
    for lead, v in leads.items():
        metadata[f'{lead}_first'] = v['first']
        metadata[f'{lead}_last'] = v['last']
    return metadata

# ---------- Loading Metadata ----------
def load_ptbxl_metadata(ptb_dir):
    csv = next((os.path.join(r, f) for r, _, fs in os.walk(ptb_dir)
                for f in fs if f == 'ptbxl_database.csv'), None)
    df = pd.read_csv(csv)
    return df[['ecg_id', 'filename_lr', 'scp_codes', 'age', 'sex']]

def load_chapman_metadata(path):
    recs = []
    for root, _, files in os.walk(path):
        for file in files:
            if file.endswith('.hea') and 'Zone' not in file:
                hea = os.path.join(root, file)
                recs.append(parse_chapman_header(hea))
    return pd.DataFrame(recs)

# ---------- Signal Loading ----------
def load_ptbxl_signal(ptb_dir, filename_lr):
    record = rdrecord(os.path.join(ptb_dir, filename_lr))
    return record.p_signal.T, record.fs  # shape: (n_leads, n_samples)

def load_chapman_signal(chap_dir, record_id):
    mat_path = os.path.join(chap_dir, f"{record_id}.mat")
    data = loadmat(mat_path)['val']  # shape: (n_leads, n_samples)
    fs = 500  # known sampling rate
    return data, fs

# ---------- Visualization ----------
def plot_signal(time, signal, lead=0, title="ECG Signal"):
    plt.figure()
    plt.plot(time, signal[lead])
    plt.xlabel('Time (s)')
    plt.ylabel('Amplitude')
    plt.title(f"{title} - Lead {lead}")
    plt.grid(True)
    plt.show()

def plot_fft(signal, fs, lead=0, title="FFT of ECG Signal"):
    n = signal.shape[1]
    yf = fft(signal[lead])
    xf = fftfreq(n, 1 / fs)
    plt.figure()
    plt.plot(xf[:n//2], np.abs(yf[:n//2]))
    plt.xlabel('Frequency (Hz)')
    plt.ylabel('Magnitude')
    plt.title(f"{title} - Lead {lead}")
    plt.grid(True)
    plt.show()

# ---------- Filtering ----------
def butter_lowpass(cutoff, fs, order=4):
    nyq = 0.5 * fs
    normal_cutoff = cutoff / nyq
    b, a = butter(order, normal_cutoff, btype='low', analog=False)
    return b, a

def apply_lowpass(data, fs, cutoff=40.0, order=4):
    b, a = butter_lowpass(cutoff, fs, order)
    return filtfilt(b, a, data)

# ---------- Main Analysis ----------
if __name__ == '__main__':
    # Paths
    PTBXL_DIR = 'data/ptb-xl'
    CHAPMAN_DIR = 'data/ChapmanShaoxing'

    # Inspect directory structures
    print("PTB-XL structure:")
    list_directory(PTBXL_DIR)
    print("\nChapman-Shaoxing structure:")
    list_directory(CHAPMAN_DIR)

    # Load metadata and filter normals
    ptb_meta = load_ptbxl_metadata(PTBXL_DIR)
    ptb_norm = ptb_meta[ptb_meta['scp_codes'].str.contains('NORM', na=False)]
    print(f"PTB-XL normales: {len(ptb_norm)} registros")

    chap_meta = load_chapman_metadata(CHAPMAN_DIR)
    chap_norm = chap_meta[chap_meta['diagnosis'] == '426177001']  # SR code
    print(f"Chapman-Shaoxing normales: {len(chap_norm)} registros")

    # Select first normal record from PTB-XL
    sample = ptb_norm.iloc[0]
    sig, fs = load_ptbxl_signal(PTBXL_DIR, sample['filename_lr'])
    time = np.arange(sig.shape[1]) / fs

    # Plot raw signal
    plot_signal(time, sig, lead=0, title="Raw PTB-XL ECG")

    # Plot FFT
    plot_fft(sig, fs, lead=0, title="Spectrum PTB-XL ECG")

    # Apply lowpass filter and plot
    filtered = apply_lowpass(sig, fs, cutoff=40.0)
    plot_signal(time, np.vstack([sig, filtered]), lead=0, title="Raw vs Filtered (Lead 0)")
    
    # Similarly for Chapman-Shaoxing
    chap_sample = chap_norm.iloc[0]
    chap_sig, chap_fs = load_chapman_signal(CHAPMAN_DIR, chap_sample['record'])
    chap_time = np.arange(chap_sig.shape[1]) / chap_fs

    plot_signal(chap_time, chap_sig, lead=0, title="Raw Chapman ECG")
    plot_fft(chap_sig, chap_fs, lead=0, title="Spectrum Chapman ECG")
    chap_filt = apply_lowpass(chap_sig, chap_fs, cutoff=40.0)
    plot_signal(chap_time, np.vstack([chap_sig, chap_filt]), lead=0, title="Raw vs Filtered Chapman")
```

--- END FILE: ecg_preprocessing_analysis.py ---

--- START FILE: ecg_project/anomaly_scores.py ---

```python
# anomaly_scores.py
import numpy as np
import torch
from sklearn.covariance import EmpiricalCovariance

def compute_elbo_score(model, loader, device, beta=1.0, sigma=1.0):
    model.eval()
    scores, z_means = [], []
    with torch.no_grad():
        for (x,) in loader:
            x = x.float().to(device)
            recon, mu, logvar = model(x)
            # recon log-likelihood
            recon_term = -0.5 * torch.sum(
                ((x - recon)**2)/(sigma**2)
                + torch.log(2 * np.pi * (sigma**2)),
                dim=[1,2]
            )
            # KL divergence
            kl_term = -0.5 * torch.sum(
                1 + logvar - mu.pow(2) - logvar.exp(),
                dim=1
            )
            elbo = recon_term - beta * kl_term
            score = -elbo
            scores.append(score.cpu().numpy())
            z_means.append(mu.cpu().numpy())
    return np.concatenate(scores), np.concatenate(z_means)


class LatentMahalanobis:
    def __init__(self):
        self.emp_cov = None
        self.mean = None

    def fit(self, z_normals):
        self.emp_cov = EmpiricalCovariance().fit(z_normals)
        self.mean = self.emp_cov.location_

    def score(self, z_batch):
        md2 = self.emp_cov.mahalanobis(z_batch)
        return np.sqrt(md2)
```

--- END FILE: ecg_project/anomaly_scores.py ---

--- START FILE: ecg_project/model.py ---

```python
"""
model.py
Define el CVAE 1D con cálculo dinámico del tamaño de flatten.
"""
import torch
import torch.nn as nn

# class CVAE(nn.Module):
#     def __init__(self, in_channels=1, latent_dim=60, input_length=2048):
#         super().__init__()
#         # Encoder: 9 capas Conv1d
#         kernels = [19]*6 + [9]*3
#         layers = []
#         ch = in_channels
#         for k_i, k in enumerate(kernels):
#             out_ch = 16 if k_i < 6 else 32
#             layers += [
#                 nn.Conv1d(ch, out_ch, k, stride=2, padding=k//2),
#                 nn.BatchNorm1d(out_ch),
#                 nn.LeakyReLU()
#             ]
#             ch = out_ch
#         self.encoder = nn.Sequential(*layers)

#         # Calcular tamaño flattened
#         length = input_length
#         for k in kernels:
#             length = (length + 2*(k//2) - (k-1) - 1)//2 + 1
#         self.flat_size = ch * length

#         # Proyecciones latentes
#         self.fc_mu     = nn.Linear(self.flat_size, latent_dim)
#         self.fc_logvar = nn.Linear(self.flat_size, latent_dim)

#         # Decoder
#         self.fc_dec = nn.Linear(latent_dim, self.flat_size)
#         rev_layers = []
#         ch_dec = ch
#         for idx, k in reversed(list(enumerate(kernels))):
#             in_ch = ch_dec
#             out_ch = in_channels if idx==0 else (16 if idx<6 else 32)
#             rev_layers += [
#                 nn.ConvTranspose1d(in_ch, out_ch, k, stride=2, padding=k//2, output_padding=1),
#                 nn.BatchNorm1d(out_ch),
#                 nn.LeakyReLU()
#             ]
#             ch_dec = out_ch
#         self.decoder = nn.Sequential(*rev_layers)

#     def reparameterize(self, mu, logvar):
#         std = torch.exp(0.5*logvar)
#         return mu + torch.randn_like(std)*std

#     def forward(self, x):
#         enc = self.encoder(x)
#         flat = enc.view(x.size(0), -1)
#         mu, logvar = self.fc_mu(flat), self.fc_logvar(flat)
#         z = self.reparameterize(mu, logvar)
#         dec_in = self.fc_dec(z).view_as(enc)
#         recon = self.decoder(dec_in)
#         return recon, mu, logvar


class CVAE(nn.Module):
    def __init__(self, in_channels=1, latent_dim=60, input_length=2048):
        super().__init__()
        # Encoder
        self.encoder = nn.Sequential(
            nn.Conv1d(in_channels, 16, kernel_size=19, stride=2, padding=9), nn.ReLU(),
            nn.Conv1d(16, 32, kernel_size=15, stride=2, padding=7), nn.ReLU(),
            nn.Conv1d(32, 64, kernel_size=11, stride=2, padding=5), nn.ReLU(),
            nn.Conv1d(64, 128, kernel_size=7, stride=2, padding=3), nn.ReLU(),
        )
        # Calcular dinámicamente flat_size con tensor dummy
        with torch.no_grad():
            dummy = torch.zeros(1, in_channels, input_length)
            enc_out = self.encoder(dummy)
            flat_size = enc_out.numel()  # 1 * channels * length

        # Flatten y proyecciones latentes
        self.flatten = nn.Flatten()
        self.fc_mu     = nn.Linear(flat_size, latent_dim)
        self.fc_logvar = nn.Linear(flat_size, latent_dim)

        # Decoder
        self.fc_dec = nn.Linear(latent_dim, flat_size)
        # Reconstrucción a partir de encoder dummy shape
        channels, length = enc_out.shape[1], enc_out.shape[2]
        self.decoder = nn.Sequential(
            nn.Unflatten(1, (channels, length)),
            nn.ConvTranspose1d(channels, 64, kernel_size=7, stride=2, padding=3, output_padding=1), nn.ReLU(),
            nn.ConvTranspose1d(64, 32, kernel_size=11, stride=2, padding=5, output_padding=1), nn.ReLU(),
            nn.ConvTranspose1d(32, 16, kernel_size=15, stride=2, padding=7, output_padding=1), nn.ReLU(),
            nn.ConvTranspose1d(16, in_channels, kernel_size=19, stride=2, padding=9, output_padding=1), nn.Sigmoid(),
        )

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def forward(self, x, epoch=None):
        enc = self.encoder(x)
        flat = self.flatten(enc)
        mu = self.fc_mu(flat)
        logvar = self.fc_logvar(flat)
        z = self.reparameterize(mu, logvar)
        dec_in = self.fc_dec(z)
        recon = self.decoder(dec_in)
        return recon, mu, logvar


def loss_function(recon_x, x, mu, logvar, beta=1.0):
    recon_loss = nn.functional.mse_loss(recon_x, x, reduction='mean')
    kld = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())
    return recon_loss + beta * kld
```

--- END FILE: ecg_project/model.py ---

--- START FILE: ecg_project/pipeline.py ---

```python
# pipeline.py
"""
Pipeline de entrenamiento, validación y test en sanos vs anomalías PTB-XL y Chapman.
Basado en Jang et al. (2021): 18 epochs, lr=1e-3, latent=60, MSE+KL, segmentos de 8.192s a 250Hz.
"""

import os
import torch
import numpy as np
import pandas as pd
from torch.utils.data import DataLoader, TensorDataset, random_split
from tqdm.auto import tqdm
from scipy.io import loadmat


# Importa TODO de tu preprocess.py
from preprocess import (
    load_ptbxl, load_chapman,             # cargas en batch
    load_ptbxl_signal,                    # carga señal individual PTB-XL
    apply_highpass, resample_signal,      # funciones de preprocesamiento
    zscore_normalize, segment_signal,
    find_file
)
from model import CVAE, loss_function
from test_functions import (
    compute_reconstruction_error,
    evaluate_detection,
    save_metrics
)


# RUTAS y HYPERPARAMS

import os

def find_data_subfolder(subfolder_name, start_path='.'):
    current_path = os.path.abspath(start_path)
    while True:
        candidate = os.path.join(current_path, 'data', subfolder_name)
        if os.path.isdir(candidate):
            return candidate
        parent = os.path.dirname(current_path)
        if parent == current_path:
            break
        current_path = parent
    return None

# Ahora buscás las rutas relativas automáticamente:
PTB_DIR = find_data_subfolder('ptb-xl/1.0.3')
CHAP_DIR = find_data_subfolder('ChapmanShaoxing')
MIT_DIR = find_data_subfolder('mitdb')


EPOCHS    = 18
BATCH     = 32
LR        = 1e-3
LATENT    = 60
MODEL_OUT = 'best_cvae.pth'
METRICS_OUT = 'metrics_combined_lead_II.json'


def preprocess_and_segment(sig, fs):
    """
    Aplica filtro, remuestreo y normalización a una señal (12, L).
    """
    sig = apply_highpass(sig, fs)
    sig = resample_signal(sig, fs)
    sig = zscore_normalize(sig)
    sig = segment_signal(sig)
    return sig


def load_ptbxl_anomalies(ptb_dir, meta_df):
    """
    Recorre el metadata DF de PTB-XL para cargar solo las señales con SCP != NORM.
    Devuelve array (N,12,2048).
    """
    out = []
    for _, row in meta_df.iterrows():
        fn = row['filename_lr']
        hea = os.path.basename(fn) + '.hea'
        hea_path = find_file(ptb_dir, hea)
        prefix = os.path.splitext(hea_path)[0]
        rec = load_ptbxl_signal(ptb_dir, fn)  # p_signal, fs
        sig, fs = rec
        if not row['scp_codes'].count('NORM'):
            sig = preprocess_and_segment(sig, fs)
            out.append(sig)
    return np.stack(out) if out else np.empty((0,12,2048))


# def load_chapman_anomalies(chap_dir):
#     """
#     Carga todas las señales Chapman y filtra solo las anomalías (Dx != 426177001).
#     """
#     out = []
#     for root, _, files in os.walk(chap_dir):
#         for f in files:
#             if f.endswith('.hea') and 'Zone' not in f:
#                 hea_path = os.path.join(root, f)
#                 # extrae Dx
#                 with open(hea_path) as fh:
#                     lines = fh.read().splitlines()
#                 dx = next((l.split()[1] for l in lines if l.startswith('#Dx:')), '')
#                 if dx != '426177001':
#                     rec_id = f.replace('.hea','')
#                     mat = os.path.join(root, rec_id + '.mat')
#                     sig = np.load(mat)['val'] if mat.endswith('.npz') else __import__('scipy.io').loadmat(mat)['val']
#                     sig = preprocess_and_segment(sig, 500)
#                     out.append(sig)
#     return np.stack(out) if out else np.empty((0,12,2048))


def load_chapman_anomalies(chap_dir):
    """
    Carga todas las señales Chapman y filtra solo las anomalías (Dx != 426177001).
    """
    out = []
    for root, _, files in os.walk(chap_dir):
        for f in files:
            if f.endswith('.hea') and 'Zone' not in f:
                hea_path = os.path.join(root, f)
                # extrae Dx
                with open(hea_path) as fh:
                    lines = fh.read().splitlines()
                dx = next((l.split()[1] for l in lines if l.startswith('#Dx:')), '')
                if dx != '426177001':
                    rec_id = f.replace('.hea','')
                    mat_path = os.path.join(root, rec_id + '.mat')
                    # aquí usamos loadmat importado
                    sig = loadmat(mat_path)['val']   # ya es (12, L)
                    sig = preprocess_and_segment(sig, 500)
                    out.append(sig)
    return np.stack(out) if out else np.empty((0,12,2048))


def load_sanos():
    # --- Carga y prepara sanos PTB+Chapman ---
    print("Cargando sanos PTB-XL y Chapman…")
    normals_ptb  = load_ptbxl(PTB_DIR, healthy_only=True)
    normals_chap = load_chapman(CHAP_DIR, healthy_only=True)
    data = np.concatenate([normals_ptb, normals_chap], axis=0)
    print(f"Total sanos disponibles: {len(data)} señales")
    
    return data

def main():
    data = load_sanos()
    model, val_loader, device= training(data)

def training(data, epochs = EPOCHS):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    # Train/Val split 80/20
    n_val = int(0.2 * len(data))
    n_train = len(data) - n_val
    train_set, val_set = random_split(data, [n_train, n_val])
    train_loader = DataLoader(TensorDataset(torch.tensor(train_set)), batch_size=BATCH, shuffle=True)
    val_loader = DataLoader(TensorDataset(torch.tensor(val_set)), batch_size=BATCH)

    # --- Modelo ---
    # model = CVAE(in_channels=1, latent_dim=LATENT, input_length=2048).to(device)
    model = CVAE(in_channels=1, latent_dim=60, input_length=2048).to(device)

    opt   = torch.optim.Adam(model.parameters(), lr=LR)
    best_mae = float('inf')

    # Entrenamiento + validación
    print("Iniciando entrenamiento…")
    for ep in range(1, epochs+1):
        model.train()
        train_loss = 0.0
<<<<<<< Updated upstream
        for (x,) in tqdm(train_loader, desc=f"Ep {ep}/{epochs} Train", leave=False):
=======
        for (x,) in tqdm(train_loader, desc=f"Ep {ep}/{EPOCHS} Train", leave=False):
            print("Input shape:", x.shape)
>>>>>>> Stashed changes
            x = x.float().to(device)
            recon, mu, logvar = model(x)
            loss = loss_function(recon, x, mu, logvar)
            opt.zero_grad(); loss.backward(); opt.step()
            train_loss += loss.item()
        train_loss /= len(train_loader)

        # Validación MAE
        model.eval()
        mae_vals = []
        with torch.no_grad():
            for (x,) in val_loader:
                x = x.float().to(device)
                recon, _, _ = model(x)
                mae_vals.append(torch.mean(torch.abs(recon - x)).item())
        val_mae = np.mean(mae_vals)

        print(f"Epoch {ep}: Train Loss={train_loss:.4f} — Val MAE={val_mae:.4f}")
        if val_mae < best_mae:
            best_mae = val_mae
            torch.save(model.state_dict(), MODEL_OUT)
            
    return model, val_loader, device



# test

def load(model, val_loader, device):
    # --- Test: sanos hold-out vs anomalías ---
    print("Cargando metadata PTB-XL para anomalías…")
    df_meta = pd.read_csv(find_file(PTB_DIR, 'ptbxl_database.csv'))
    # # Hold-out sanos
    # healthy_errors = compute_reconstruction_error(model, val_loader, device)
    # print(f"Hold-out sanos MAE mean={healthy_errors.mean():.4f}, std={healthy_errors.std():.4f}")
    
    # hold-out sanos
    healthy_errors = compute_reconstruction_error(model, val_loader, device)
    thr = np.percentile(healthy_errors, 12.16)
    print(f"Threshold @12pct de sanos: {thr:.4f}")

    
    return df_meta, healthy_errors, thr


def anomalos_ptb(df_meta, model, device):
    # Anómalos PTB-XL
    anom_ptb = load_ptbxl_anomalies(PTB_DIR, df_meta)
    ptb_loader = DataLoader(TensorDataset(torch.tensor(anom_ptb)), batch_size=BATCH)
    ptb_errors = compute_reconstruction_error(model, ptb_loader, device)
    print(f"Anómalos PTB-XL: {len(anom_ptb)} señales")
    
    return ptb_errors
    

def anomalos_chap(model, device):

    # Anómalos Chapman
    anom_chap = load_chapman_anomalies(CHAP_DIR)
    chap_loader = DataLoader(TensorDataset(torch.tensor(anom_chap)), batch_size=BATCH)
    chap_errors = compute_reconstruction_error(model, chap_loader, device)
    print(f"Anómalos Chapman: {len(anom_chap)} señales")
    
    return chap_errors


def metrics(healthy_errors, ptb_errors, chap_errors):

    # Métricas combinadas
    y_true = np.concatenate([
        np.zeros_like(healthy_errors),
        np.ones_like(ptb_errors),
        np.ones_like(chap_errors)
    ])
    y_pred = np.concatenate([
        healthy_errors,
        ptb_errors,
        chap_errors
    ]) > (healthy_errors.mean() + 3 * healthy_errors.std())

    metrics = evaluate_detection(y_true, y_pred)
    print("Métricas final (sanos vs PTB+Chapman anomalías):", metrics)
    save_metrics(metrics, METRICS_OUT)
    print(f"Resultados guardados en {METRICS_OUT}")


def metrics_fixed_threshold(healthy_errors, ptb_errors, chap_errors, threshold):
    y_true = np.concatenate([
        np.zeros_like(healthy_errors),
        np.ones_like(ptb_errors),
        np.ones_like(chap_errors)
    ])
    y_pred = np.concatenate([
        healthy_errors,
        ptb_errors,
        chap_errors
    ]) > threshold

    metrics = evaluate_detection(y_true, y_pred)
    print("Métricas (con umbral percentil 12 de sanos):", metrics)
    save_metrics(metrics, METRICS_OUT)
    print(f"Resultados guardados en {METRICS_OUT}")
    

if __name__ == '__main__':
    main()
```

--- END FILE: ecg_project/pipeline.py ---

--- START FILE: ecg_project/pipeline_improved.py ---

```python
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset, Subset
from sklearn.metrics import roc_auc_score
from sklearn.ensemble import IsolationForest
from sklearn.svm import OneClassSVM
from xgboost import XGBClassifier
import pandas as pd

from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, accuracy_score, r2_score



# from preprocess import (
#     load_sanos,
#     load_ptbxl_anomalies,
#     load_chapman_anomalies
# )

from preprocess import (
    load_sanos,
    load_all_anomalies,
    compute_zscore_stats,
    apply_zscore,
)

# Augmentation
def augment_ecg(sig):
    sig = sig + 0.01 * np.random.randn(*sig.shape)
    scale = np.random.uniform(0.8, 1.2)
    return sig * scale

# Beta schedules
def beta_linear(epoch, epochs):
    return 1.0 + 4.0 * (epoch / (epochs-1))

def beta_cyclic(epoch, cycle=10, beta_max=4.0):
    phase = epoch % cycle
    return beta_max * (phase / (cycle-1))

# Residual block
class ResBlock1D(nn.Module):
    def __init__(self, ch, kernel_size, dilation):
        super().__init__()
        padding = (kernel_size//2) * dilation
        self.block = nn.Sequential(
            nn.Conv1d(ch, ch, kernel_size, padding=padding, dilation=dilation), nn.LeakyReLU(),
            nn.Conv1d(ch, ch, kernel_size, padding=padding, dilation=dilation)
        )
    def forward(self, x):
        return x + self.block(x)

# VAE model
torch.manual_seed(0)
class VAE1D(nn.Module):
    def __init__(self, input_ch=1, latent_dim=16, seq_len=2048, n_blocks=3):
        super().__init__()
        layers = [nn.Conv1d(input_ch, 32, 19, 2, 9), nn.LeakyReLU()]
        for i in range(n_blocks):
            layers.append(ResBlock1D(32, 3, 2**i))
        self.encoder = nn.Sequential(*layers)
        self.out_len = seq_len // 2
        flat_size = 32 * self.out_len
        self.flatten = nn.Flatten()
        self.fc_mu = nn.Linear(flat_size, latent_dim)
        self.fc_logv = nn.Linear(flat_size, latent_dim)
        self.fc_dec = nn.Linear(latent_dim, flat_size)
        self.decoder = nn.Sequential(
            nn.Unflatten(1, (32, self.out_len)),
            nn.ConvTranspose1d(32, input_ch, 19, 2, 9, output_padding=1)
        )

    def encode(self, x):
        h = self.encoder(x)
        return self.fc_mu(self.flatten(h)), self.fc_logv(self.flatten(h))

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        return mu + std * torch.randn_like(std)

    def decode(self, z):
        return self.decoder(self.fc_dec(z))

    def forward(self, x):
        mu, logv = self.encode(x)
        return mu, logv

# Compute scores
def compute_scores(model, loader, device, beta):
    model.eval()
    errs, zs = [], []
    with torch.no_grad():
        for x, _ in loader:
            x = x.to(device)
            mu, logv = model.encode(x)
            z = model.reparameterize(mu, logv)
            rec = model.decode(z)
            mse = ((rec - x)**2).mean(dim=[1,2]).cpu().numpy()
            kl = (-0.5 * (1 + logv - mu.pow(2) - logv.exp()).sum(dim=1)).cpu().numpy()
            elbo = -mse - beta * kl
            errs.append(elbo)
            zs.append(mu.cpu().numpy())
    return np.concatenate(errs), np.vstack(zs)

# # Training and evaluation with best hyperparameters
# if __name__ == '__main__':
#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
#     # 1) Load and prepare data
#     norms = load_sanos()
#     ptb   = load_ptbxl_anomalies()
#     chap  = load_chapman_anomalies()
#     anos  = np.concatenate([ptb, chap], axis=0)

#     # Select lead II and reshape
#     norms_lead = norms[:, 0, :]
#     anos_lead  = anos[:, 0, :]
#     X_norm = torch.tensor(norms_lead, dtype=torch.float32).unsqueeze(1)
#     X_ano  = torch.tensor(anos_lead,  dtype=torch.float32).unsqueeze(1)

#     # Split normals
#     n_norm = len(X_norm)
#     i1 = int(0.8 * n_norm)
#     train_norm = X_norm[:i1]
#     val_norm   = X_norm[i1:]

#     # Chosen hyperparameters
#     latent_dim = 32
#     lr         = 1e-3
#     n_blocks   = 3
#     beta_fn    = beta_cyclic
#     epochs     = 50

#     # a) Train VAE on normals only
#     train_ds    = TensorDataset(train_norm, torch.zeros(len(train_norm)))
#     train_loader= DataLoader(train_ds, batch_size=32, shuffle=True)
#     model = VAE1D(input_ch=1, latent_dim=latent_dim, n_blocks=n_blocks).to(device)
#     optimizer = torch.optim.Adam(model.parameters(), lr=lr)
#     for epoch in range(epochs):
#         model.train()
#         total_loss = 0
#         for x, _ in train_loader:
#             x = x.to(device)
#             mu, logv = model.encode(x)
#             z = model.reparameterize(mu, logv)
#             rec = model.decode(z)
#             recon_loss = ((rec - x)**2).mean()
#             kl_loss = (-0.5*(1+logv-mu.pow(2)-logv.exp()).sum())/x.size(0)
#             beta = beta_fn(epoch, epochs)
#             loss = recon_loss + beta * kl_loss
#             optimizer.zero_grad(); loss.backward(); optimizer.step()
#             total_loss += loss.item()
#         print(f"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}")

#     # b) Prepare balanced test set
#     N = len(val_norm)
#     val_ano = X_ano[:N]
#     X_test  = torch.cat([val_norm, val_ano], dim=0)
#     y_test  = np.concatenate([np.zeros(N), np.ones(N)])
#     test_ds = TensorDataset(X_test, torch.tensor(y_test, dtype=torch.long))
#     test_loader = DataLoader(test_ds, batch_size=32, shuffle=False)

#     # c) Compute ELBO scores and latents
#     beta_last = beta_fn(epochs-1, epochs)
#     errs, zs = compute_scores(model, test_loader, device, beta_last)

#     # d) Mahalanobis distance
#     mu_bar = zs[:N].mean(axis=0)
#     cov = np.cov(zs[:N].T) + 1e-6*np.eye(latent_dim)
#     invcov = np.linalg.inv(cov)
#     dM = np.sqrt(((zs-mu_bar)@invcov*(zs-mu_bar)).sum(axis=1))

#     # e) Combine with alpha=1 (ELBO) or tuned alpha, here use best_alpha from sweep
#     alpha = 1.0
#     combined_score = alpha*errs + (1-alpha)*dM

#     # f) XGBoost classifier on [err, latents]
#     Xf = np.hstack([errs.reshape(-1,1), zs])
#     clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss')
#     clf.fit(Xf, y_test)
#     prob = clf.predict_proba(Xf)[:,1]

#     # g) Compute metrics
#     from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, r2_score
#     # Binarize at 0.5 for classification metrics
#     y_pred = (prob >= 0.5).astype(int)
#     metrics = {
#         'roc_auc': roc_auc_score(y_test, prob),
#         'precision': precision_score(y_test, y_pred),
#         'recall': recall_score(y_test, y_pred),
#         'f1': f1_score(y_test, y_pred),
#         'accuracy': accuracy_score(y_test, y_pred),
#         'r2': r2_score(y_test, prob)
#     }
#     print("Final metrics:", metrics)
import os

def find_data_subfolder(subfolder_name, start_path='.'):
    current_path = os.path.abspath(start_path)
    while True:
        candidate = os.path.join(current_path, 'data', subfolder_name)
        if os.path.isdir(candidate):
            return candidate
        parent = os.path.dirname(current_path)
        if parent == current_path:
            break
        current_path = parent
    return None

def find_file(root, filename):
    """
    Busca recursivamente `filename` bajo `root` y devuelve la ruta completa.
    """
    for r, _, files in os.walk(root):
        if filename in files:
            return os.path.join(r, filename)
    return None


# Ahora buscás las rutas relativas automáticamente:
PTB_DIR = find_data_subfolder('ptb-xl/1.0.3')
CHAP_DIR = find_data_subfolder('ChapmanShaoxing')
MIT_DIR = find_data_subfolder('mitdb')





if __name__ == '__main__':
    # 1) Cargar datos sanos y anomalías
    normals   = load_sanos(PTB_DIR, CHAP_DIR)  # (N_norm,1,L)
    ptb_df    = pd.read_csv(find_file(PTB_DIR, 'ptbxl_database.csv'))
    anomalies = load_all_anomalies(PTB_DIR, CHAP_DIR, ptb_df)  # (N_ano,1,L)

    # 2) Split normales en train/dev/test (60/20/20)
    n_norm = normals.shape[0]
    i1 = int(0.6 * n_norm)
    i2 = int(0.8 * n_norm)
    train_norm = normals[:i1]
    val_norm   = normals[i1:i2]
    test_norm  = normals[i2:]

    # 3) Normalización Z-score usando solo train_norm
    mean, std = compute_zscore_stats(train_norm)
    train_norm = apply_zscore(train_norm, mean, std)
    val_norm   = apply_zscore(val_norm,   mean, std)
    test_norm  = apply_zscore(test_norm,  mean, std)
    anomalies  = apply_zscore(anomalies,  mean, std)

    # 4) Convertir a tensores
    train_tensor = torch.tensor(train_norm, dtype=torch.float32)
    val_tensor   = torch.tensor(val_norm,   dtype=torch.float32)
    test_tensor  = torch.tensor(test_norm,  dtype=torch.float32)
    ano_tensor   = torch.tensor(anomalies,  dtype=torch.float32)
    
    # 4) Configuración entrenamiento único
    latent_dim = 32
    lr         = 1e-3
    n_blocks   = 3
    epochs     = 50
    batch_size = 32

    # 5) Entrena VAE sobre conjunto DEV (train_norm + val_norm)
    dev_tensor = torch.cat([train_tensor, val_tensor], dim=0)
    dev_ds     = TensorDataset(dev_tensor, torch.zeros(len(dev_tensor)))
    dev_loader = DataLoader(dev_ds, batch_size=batch_size, shuffle=True)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = VAE1D(input_ch=1, latent_dim=latent_dim, n_blocks=n_blocks).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    for epoch in range(epochs):
        model.train()
        total_loss = 0.0
        for x, _ in dev_loader:
            x = x.to(device)
            mu, logv = model.encode(x)
            z = model.reparameterize(mu, logv)
            rec = model.decode(z)
            recon_loss = ((rec - x)**2).mean()
            kl_loss = (-0.5 * (1 + logv - mu.pow(2) - logv.exp()).sum()) / x.size(0)
            beta = beta_cyclic(epoch, cycle=10, beta_max=4.0)
            loss = recon_loss + beta * kl_loss
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dev_loader):.4f}")

    # 6) Prepara test final balanceado: test_norm vs primeras anomalías
    N_test = test_tensor.shape[0]
    test_x = torch.cat([test_tensor, ano_tensor[:N_test]], dim=0)
    y_test = np.concatenate([np.zeros(N_test), np.ones(N_test)])
    test_loader = DataLoader(TensorDataset(test_x, torch.tensor(y_test, dtype=torch.long)), batch_size=batch_size)

    # 7) Obtiene scores ELBO y z_mean en test
    beta_last = beta_cyclic(epochs-1, cycle=10, beta_max=4.0)
    errs, zs = compute_scores(model, test_loader, device, beta_last)

    # 8) Mahalanobis distance en test_norm
    mu_bar = zs[:N_test].mean(axis=0)
    cov    = np.cov(zs[:N_test].T) + 1e-6 * np.eye(latent_dim)
    invcov = np.linalg.inv(cov)
    dM     = np.sqrt(((zs - mu_bar) @ invcov * (zs - mu_bar)).sum(axis=1))

    # 9) Clasificador XGBoost en test
    # Limpiar infinities
    max_val = np.finfo(np.float32).max/10
    min_val = np.finfo(np.float32).min/10
    errs = np.nan_to_num(errs, nan=0.0, posinf=max_val, neginf=min_val)
    zs   = np.nan_to_num(zs,   nan=0.0, posinf=max_val, neginf=min_val)
    Xf   = np.hstack([errs.reshape(-1,1), zs])
    clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss')
    clf.fit(Xf, y_test)
    probs = clf.predict_proba(Xf)[:,1]

    # 10) Métricas finales
    auc      = roc_auc_score(y_test, probs)
    y_pred   = (probs >= 0.5).astype(int)
    metrics  = {
        'roc_auc':   auc,
        'precision': precision_score(y_test, y_pred),
        'recall':    recall_score(y_test, y_pred),
        'f1':        f1_score(y_test, y_pred),
        'accuracy':  accuracy_score(y_test, y_pred),
        'r2':        r2_score(y_test, probs)
    }
    print('Final metrics:', metrics)
```

--- END FILE: ecg_project/pipeline_improved.py ---

--- START FILE: ecg_project/preprocess.py ---

```python
import os
import numpy as np
import pandas as pd
from scipy.signal import butter, filtfilt, resample
import wfdb
from wfdb import rdrecord
from scipy.io import loadmat
import os
import numpy as np
import pandas as pd
import torch
from torch.utils.data import DataLoader, TensorDataset
from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, accuracy_score, r2_score
from xgboost import XGBClassifier


# Global parameters
target_fs       = 250      # Desired sampling frequency
segment_sec     = 8.192    # Seconds per segment
segment_samples = int(target_fs * segment_sec)  # ~2048 samples

# --- File finder ---
def find_file(root: str, filename: str) -> str:
    """
    Recursively search for `filename` under `root` directory.
    """
    for dirpath, _, files in os.walk(root):
        if filename in files:
            return os.path.join(dirpath, filename)
    raise FileNotFoundError(f"{filename} not found under {root}")

# --- Preprocessing building blocks ---
def butter_highpass(cutoff: float, fs: float, order: int = 4):
    nyq = 0.5 * fs
    normal_cutoff = cutoff / nyq
    return butter(order, normal_cutoff, btype='high', analog=False)


def apply_highpass(sig: np.ndarray, fs: float, cutoff: float = 0.5, order: int = 4) -> np.ndarray:
    """Highpass filter channel-wise to remove baseline drift."""
    b, a = butter_highpass(cutoff, fs, order)
    return filtfilt(b, a, sig, axis=1)


def resample_signal(sig: np.ndarray, orig_fs: float) -> np.ndarray:
    """Resample multichannel signal to target_fs."""
    n_samples = int(sig.shape[1] * target_fs / orig_fs)
    return resample(sig, n_samples, axis=1)


def segment_signal(sig: np.ndarray) -> np.ndarray:
    """Segment to fixed length, selecting second lead (index 1)."""
    # Expect sig shape (n_leads, L)
    if sig.shape[0] > 1:
        lead = sig[1:2, :]
    else:
        lead = sig
    L = lead.shape[1]
    if L >= segment_samples:
        return lead[:, :segment_samples]
    pad = segment_samples - L
    return np.pad(lead, ((0,0),(0,pad)), mode='constant')


def preprocess_basic(sig: np.ndarray, fs: float) -> np.ndarray:
    """Filter, resample, and segment, but do not normalize."""
    x = apply_highpass(sig, fs)
    x = resample_signal(x, fs)
    x = segment_signal(x)
    return x

# --- Z-score normalization (train stats only) ---
def compute_zscore_stats(signals: np.ndarray):
    """
    Compute per-channel mean/std for array of shape (N,1,L).
    Returns mean and std arrays of shape (1,1,L).
    """
    mean = np.mean(signals, axis=(0,2), keepdims=True)
    std  = np.std(signals,  axis=(0,2), keepdims=True) + 1e-6
    return mean, std


def apply_zscore(signals: np.ndarray, mean: np.ndarray, std: np.ndarray) -> np.ndarray:
    """Apply Z-score using provided mean and std."""
    return (signals - mean) / std

# --- PTB-XL loading ---
def load_ptbxl_signal(ptb_dir: str, filename_lr: str):
    """Load one PTB-XL record, return (12,L) and sampling rate."""
    # Find header
    hea_path = find_file(ptb_dir, os.path.basename(filename_lr) + '.hea')
    rec = rdrecord(os.path.splitext(hea_path)[0])
    sig = rec.p_signal.T  # (12, L)
    fs = rec.fs
    return sig, fs


def load_ptbxl(ptb_dir: str, healthy_only: bool = True) -> np.ndarray:
    """Load all PTB-XL signals, filter healthy if requested."""
    csv_path = find_file(ptb_dir, 'ptbxl_database.csv')
    df = pd.read_csv(csv_path)
    if healthy_only:
        df = df[df['scp_codes'].str.contains('NORM', na=False)]
    records = []
    for _, row in df.iterrows():
        sig, fs = load_ptbxl_signal(ptb_dir, row['filename_lr'])
        records.append(preprocess_basic(sig, fs))
    return np.stack(records, axis=0)  # (N,1,L)

# --- Chapman-Shaoxing loading ---
def load_chapman(chap_dir: str, healthy_only: bool = True) -> np.ndarray:
    """Load Chapman-Shaoxing signals, healthy_only filters sinus rhythm."""
    records = []
    for root, _, files in os.walk(chap_dir):
        for f in files:
            if not f.endswith('.hea') or 'Zone' in f:
                continue
            hea_path = os.path.join(root, f)
            with open(hea_path) as fh:
                lines = fh.read().splitlines()
            dx = next((l.split()[1] for l in lines if l.startswith('#Dx:')), '')
            if healthy_only and dx != '426177001':
                continue
            mat_path = os.path.join(root, f.replace('.hea','.mat'))
            val = loadmat(mat_path)['val']  # (12,L)
            records.append(preprocess_basic(val, 500))
    return np.stack(records, axis=0) if records else np.empty((0,1,segment_samples))

# --- MIT-BIH loading (example) ---
def load_mitbih(mit_dir: str) -> np.ndarray:
    """Load MIT-BIH arrhythmia database signals."""
    records = []
    for root, _, files in os.walk(mit_dir):
        for f in files:
            if not f.endswith('.hea'):
                continue
            rec = rdrecord(os.path.splitext(os.path.join(root,f))[0])
            sig = rec.p_signal.T
            records.append(preprocess_basic(sig, rec.fs))
    return np.stack(records, axis=0) if records else np.empty((0,1,segment_samples))

# --- Combined loader for healthy normals ---
def load_sanos(ptb_dir: str, chap_dir: str) -> np.ndarray:
    """
    Load all healthy signals from PTB-XL and Chapman-Shaoxing.
    Returns array of shape (N,1,segment_samples).
    """
    ptb = load_ptbxl(ptb_dir, healthy_only=True)
    chap = load_chapman(chap_dir, healthy_only=True)
    if ptb.size == 0 and chap.size == 0:
        return np.empty((0,1,segment_samples))
    return np.concatenate([ptb, chap], axis=0)


# --- Combined loader for all anomalies ---
def load_all_anomalies(ptb_dir: str, chap_dir: str, meta_df: pd.DataFrame) -> np.ndarray:
    """
    Load all anomalous signals from PTB-XL (using meta_df) and Chapman-Shaoxing.
    """
    ptb_ano = load_ptbxl(ptb_dir, healthy_only=False)
    chap_ano = load_chapman(chap_dir, healthy_only=False)
    if ptb_ano.size == 0 and chap_ano.size == 0:
        return np.empty((0,1,segment_samples))
    return np.concatenate([ptb_ano, chap_ano], axis=0)
```

--- END FILE: ecg_project/preprocess.py ---

--- START FILE: ecg_project/test_functions.py ---

```python
"""
test_functions.py
Functions to evaluate the CVAE.
"""
import numpy as np
import torch
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score


# def compute_reconstruction_error(model, loader, device):
#     model.eval()
#     errors = []
#     with torch.no_grad():
#         for (x,) in loader:
#             x = x.to(device).float()  # <- ESTE CASTEO ES CLAVE
#             recon, _, _ = model(x)
#             err = torch.mean((recon - x) ** 2, dim=[1, 2])  # por señal
#             errors.append(err.cpu().numpy())
#     return np.concatenate(errors)


def compute_reconstruction_error(model, loader, device):
    model.eval()
    errs = []
    with torch.no_grad():
        for (x,) in loader:
            x = x.to(device).float()
            recon, _, _ = model(x)
            # error absoluto medio por muestra
            err = torch.mean(torch.abs(recon - x), dim=[1,2])
            errs.append(err.cpu().numpy())
    return np.concatenate(errs)


def evaluate_detection(y_true, y_pred):
    return {
        'precision': precision_score(y_true, y_pred),
        'recall': recall_score(y_true, y_pred),
        'f1': f1_score(y_true, y_pred),
        'auc': roc_auc_score(y_true, y_pred)
    }

def save_metrics(metrics, outfile):
    import json
    with open(outfile, 'w') as f:
        json.dump(metrics, f, indent=2)


# test_functions.py (o un nuevo módulo de scores)

import torch
import numpy as np

def compute_anomaly_scores(model, loader, device, beta=1.0, sigma=1.0):
    """
    Para cada muestra devuelve un diccionario con:
      mse, logp, kl, elbo, score
    """
    model.eval()
    all_scores = {k: [] for k in ('mse','logp','kl','elbo','score')}
    with torch.no_grad():
        for (x,) in loader:
            x = x.float().to(device)
            recon, mu, logvar = model(x)
            # 1) MSE por muestra
            mse = torch.mean((recon-x)**2, dim=[1,2])
            # 2) recon log-prob (Gaussiano isotrópico σ)
            recon_logprob = -0.5 * torch.sum(
                (recon - x)**2 / sigma**2 + torch.log(2*np.pi*sigma**2),
                dim=[1,2]
            )
            # 3) kl divergencia
            kl = -0.5 * torch.sum(1 + logvar - mu**2 - torch.exp(logvar), dim=1)
            # 4) elbo y score
            elbo = recon_logprob - beta * kl
            score = -elbo
            # guardar
            for k, v in zip(('mse','logp','kl','elbo','score'),
                            (mse, recon_logprob, kl, elbo, score)):
                all_scores[k].append(v.cpu().numpy())
    # concatenar
    for k in all_scores:
        all_scores[k] = np.concatenate(all_scores[k])
    return all_scores
```

--- END FILE: ecg_project/test_functions.py ---

--- START FILE: ecg_project/utils/metrics.py ---

```python
import numpy as np
from sklearn.metrics import roc_curve, precision_recall_curve, auc

def compute_roc(y_true, scores):
    """
    Compute ROC curve and AUC.
    Returns fpr, tpr, thresholds, roc_auc
    """
    fpr, tpr, thresholds = roc_curve(y_true, scores)
    roc_auc = auc(fpr, tpr)
    return fpr, tpr, thresholds, roc_auc


def compute_pr(y_true, scores):
    """
    Compute Precision-Recall curve and AUC.
    Returns precision, recall, thresholds, pr_auc
    """
    precision, recall, thresholds = precision_recall_curve(y_true, scores)
    pr_auc = auc(recall, precision)
    return precision, recall, thresholds, pr_auc


def optimal_threshold(fpr, tpr, thresholds):
    """
    Compute optimal threshold using Youden's J statistic.
    """
    j_scores = tpr - fpr
    idx = np.argmax(j_scores)
    return thresholds[idx]
```

--- END FILE: ecg_project/utils/metrics.py ---